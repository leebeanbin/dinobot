"""
MongoDB ê³ ê¸‰ í™œìš© ì„œë¹„ìŠ¤
- ì§‘ê³„ íŒŒì´í”„ë¼ì¸ì„ í†µí•œ ë³µì¡í•œ ë°ì´í„° ë¶„ì„
- ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ë° Change Streams
- ìë™ ë°ì´í„° ì •ë¦¬ ë° ì•„ì¹´ì´ë¹™
- ì„±ëŠ¥ ìµœì í™” ë° ì¸ë±ìŠ¤ ê´€ë¦¬
- ë°ì´í„° ë°±ì—… ë° ë³µêµ¬
"""

from typing import Dict, Any, List, Optional, AsyncGenerator
from datetime import datetime, timedelta
import asyncio
from pymongo import DESCENDING, ASCENDING
from motor.motor_asyncio import AsyncIOMotorCollection, AsyncIOMotorChangeStream

from core.config import settings
from core.logger import get_logger, logger_manager
from core.database import mongodb_connection
from core.exceptions import DatabaseOperationException, safe_execution
from core.decorators import track_mongodb_query
from models.dtos import (
    CommandExecutionMetricDTO,
    APICallMetricDTO,
    CachePerformanceMetricDTO,
    SystemStatusDTO,
)

# Module logger
logger = get_logger("services.mongodb_advanced")


class MongoDBAnalysisService:
    """
    MongoDB ì§‘ê³„ íŒŒì´í”„ë¼ì¸ì„ í™œìš©í•œ ê³ ê¸‰ ë°ì´í„° ë¶„ì„

    ì£¼ìš” ê¸°ëŠ¥:
    - ì‹¤ì‹œê°„ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±
    - ì‚¬ìš©ìë³„/ëª…ë ¹ì–´ë³„ í†µê³„ ë¶„ì„
    - íŠ¸ë Œë“œ ë¶„ì„ ë° ì˜ˆì¸¡
    - ì´ìƒ ì§•í›„ íƒì§€
    """

    def __init__(self):
        self.metrics_collection = mongodb_connection.metrics_collection
        self.schema_cache_collection = mongodb_connection.schema_cache_collection
        self.thread_cache_collection = mongodb_connection.thread_cache_collection

    @safe_execution("generate_realtime_dashboard_data")
    async def generate_realtime_dashboard_data(self) -> Dict[str, Any]:
        """
        ì‹¤ì‹œê°„ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œë¥¼ ìœ„í•œ ì¢…í•© ë°ì´í„° ìƒì„±

        Returns:
            Dict: ëŒ€ì‹œë³´ë“œìš© ì‹¤ì‹œê°„ ë°ì´í„°
        """
        try:
            with logger_manager.performance_logger("dashboard_data_aggregation"):
                # ë™ì‹œì— ì—¬ëŸ¬ ë¶„ì„ ì‹¤í–‰
                results = await asyncio.gather(
                    self._get_recent_1hour_command_stats(),
                    self._analyze_realtime_response_time(),
                    self._analyze_cache_performance(),
                    self._analyze_error_trends(),
                    self._analyze_user_activity(),
                    return_exceptions=True,
                )

                # ê²°ê³¼ í†µí•©
                dashboard_data = {
                    "timestamp": datetime.now().isoformat(),
                    "command_stats": (
                        results[0] if not isinstance(results[0], Exception) else {}
                    ),
                    "response_time_analysis": (
                        results[1] if not isinstance(results[1], Exception) else {}
                    ),
                    "cache_performance": (
                        results[2] if not isinstance(results[2], Exception) else {}
                    ),
                    "error_trends": (
                        results[3] if not isinstance(results[3], Exception) else {}
                    ),
                    "user_activity": (
                        results[4] if not isinstance(results[4], Exception) else {}
                    ),
                }

                logger.debug("ğŸ“Š ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„± ì™„ë£Œ")
                return dashboard_data

        except Exception as analysis_error:
            raise DatabaseOperationException(
                "ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„± ì‹¤íŒ¨", original_exception=analysis_error
            )

    async def _get_recent_1hour_command_stats(self) -> Dict[str, Any]:
        """ìµœê·¼ 1ì‹œê°„ ëª…ë ¹ì–´ ì‚¬ìš© í†µê³„"""
        start_time = datetime.now() - timedelta(hours=1)

        pipeline = [
            {"$match": {"type": "command_usage", "timestamp": {"$gte": start_time}}},
            {
                "$group": {
                    "_id": "$command",
                    "total_executions": {"$sum": 1},
                    "successful_executions": {
                        "$sum": {"$cond": [{"$eq": ["$success", True]}, 1, 0]}
                    },
                    "avg_execution_time": {"$avg": "$execution_time"},
                    "max_execution_time": {"$max": "$execution_time"},
                    "min_execution_time": {"$min": "$execution_time"},
                }
            },
            {
                "$addFields": {
                    "success_rate": {
                        "$multiply": [
                            {
                                "$divide": [
                                    "$successful_executions",
                                    "$total_executions",
                                ]
                            },
                            100,
                        ]
                    }
                }
            },
            {"$sort": {"total_executions": DESCENDING}},
        ]

        result = await self.metrics_collection.aggregate(pipeline).to_list(100)

        return {
            "period": f"ìµœê·¼ 1ì‹œê°„ ({start_time.strftime('%H:%M')} ~ {datetime.now().strftime('%H:%M')})",
            "command_statistics": result,
            "total_commands": sum(item["total_executions"] for item in result),
            "average_success_rate": (
                sum(item["success_rate"] for item in result) / len(result)
                if result
                else 0
            ),
        }

    async def _analyze_realtime_response_time(self) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ì‘ë‹µ ì‹œê°„ ë¶„ì„ (5ë¶„ ë‹¨ìœ„)"""
        start_time = datetime.now() - timedelta(minutes=30)

        pipeline = [
            {
                "$match": {
                    "timestamp": {"$gte": start_time},
                    "execution_time": {"$exists": True},
                }
            },
            {
                "$group": {
                    "_id": {
                        # 5ë¶„ ë‹¨ìœ„ë¡œ ê·¸ë£¹í•‘
                        "year": {"$year": "$timestamp"},
                        "month": {"$month": "$timestamp"},
                        "day": {"$dayOfMonth": "$timestamp"},
                        "hour": {"$hour": "$timestamp"},
                        "five_min_interval": {
                            "$multiply": [
                                {"$floor": {"$divide": [{"$minute": "$timestamp"}, 5]}},
                                5,
                            ]
                        },
                    },
                    "avg_response_time": {"$avg": "$execution_time"},
                    "max_response_time": {"$max": "$execution_time"},
                    "request_count": {"$sum": 1},
                }
            },
            {
                "$addFields": {
                    "time_period": {
                        "$dateFromParts": {
                            "year": "$_id.year",
                            "month": "$_id.month",
                            "day": "$_id.day",
                            "hour": "$_id.hour",
                            "minute": "$_id.five_min_interval",
                        }
                    }
                }
            },
            {"$sort": {"time_period": ASCENDING}},
        ]

        result = await self.metrics_collection.aggregate(pipeline).to_list(100)

        return {
            "hourly_response_times": result,
            "overall_avg_response_time": (
                sum(item["avg_response_time"] for item in result) / len(result)
                if result
                else 0
            ),
            "max_response_time": max(
                (item["max_response_time"] for item in result), default=0
            ),
        }

    async def _analyze_cache_performance(self) -> Dict[str, Any]:
        """ìºì‹œ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„"""
        start_time = datetime.now() - timedelta(hours=1)

        # ìŠ¤í‚¤ë§ˆ ìºì‹œ ë¶„ì„
        schema_cache_pipeline = [
            {
                "$group": {
                    "_id": None,
                    "total_cache_count": {"$sum": 1},
                    "avg_cache_size": {"$avg": {"$bsonSize": "$$ROOT"}},
                    "last_update": {"$max": "$updated_at"},
                }
            }
        ]

        schema_cache_result = await self.schema_cache_collection.aggregate(
            schema_cache_pipeline
        ).to_list(1)

        # ìŠ¤ë ˆë“œ ìºì‹œ ë¶„ì„
        thread_cache_pipeline = [
            {
                "$group": {
                    "_id": None,
                    "total_thread_count": {"$sum": 1},
                    "avg_usage_count": {"$avg": "$use_count"},
                    "total_usage_count": {"$sum": "$use_count"},
                    "active_thread_count": {
                        "$sum": {"$cond": [{"$gte": ["$last_used", start_time]}, 1, 0]}
                    },
                }
            }
        ]

        thread_cache_result = await self.thread_cache_collection.aggregate(
            thread_cache_pipeline
        ).to_list(1)

        return {
            "schema_cache": schema_cache_result[0] if schema_cache_result else {},
            "thread_cache": thread_cache_result[0] if thread_cache_result else {},
            "cache_efficiency": (
                "high"
                if thread_cache_result
                and thread_cache_result[0].get("avg_usage_count", 0) > 2
                else "normal"
            ),
        }

    async def _analyze_error_trends(self) -> Dict[str, Any]:
        """ì—ëŸ¬ ë°œìƒ íŒ¨í„´ ë° ì¶”ì´ ë¶„ì„"""
        start_time = datetime.now() - timedelta(hours=24)

        pipeline = [
            {"$match": {"timestamp": {"$gte": start_time}, "success": False}},
            {
                "$group": {
                    "_id": {"hour": {"$hour": "$timestamp"}, "type": "$type"},
                    "error_count": {"$sum": 1},
                }
            },
            {"$sort": {"_id.hour": ASCENDING}},
        ]

        result = await self.metrics_collection.aggregate(pipeline).to_list(100)

        # ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ ì§‘ê³„
        hourly_errors = {}
        for item in result:
            hour = item["_id"]["hour"]
            if hour not in hourly_errors:
                hourly_errors[hour] = 0
            hourly_errors[hour] += item["error_count"]

        return {
            "24hour_error_trends": hourly_errors,
            "total_error_count": sum(item["error_count"] for item in result),
            "error_patterns": result,
        }

    async def _analyze_user_activity(self) -> Dict[str, Any]:
        """ì‚¬ìš©ì í™œë™ íŒ¨í„´ ë¶„ì„"""
        start_time = datetime.now() - timedelta(hours=24)

        pipeline = [
            {"$match": {"type": "command_usage", "timestamp": {"$gte": start_time}}},
            {
                "$group": {
                    "_id": "$user_id",
                    "command_executions": {"$sum": 1},
                    "used_commands": {"$addToSet": "$command"},
                    "first_execution": {"$min": "$timestamp"},
                    "last_execution": {"$max": "$timestamp"},
                    "avg_execution_time": {"$avg": "$execution_time"},
                }
            },
            {
                "$addFields": {
                    "activity_duration_minutes": {
                        "$divide": [
                            {"$subtract": ["$last_execution", "$first_execution"]},
                            60000,  # ë°€ë¦¬ì´ˆë¥¼ ë¶„ìœ¼ë¡œ ë³€í™˜
                        ]
                    },
                    "unique_command_count": {"$size": "$used_commands"},
                }
            },
            {"$sort": {"command_executions": DESCENDING}},
            {"$limit": 20},  # ìƒìœ„ 20ëª…ë§Œ
        ]

        result = await self.metrics_collection.aggregate(pipeline).to_list(20)

        return {
            "active_user_count": len(result),
            "top_users": result,
            "total_command_executions": sum(
                item["command_executions"] for item in result
            ),
        }


class MongoDBRealTimeStreaming:
    """
    MongoDB Change Streamsë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°

    ì£¼ìš” ê¸°ëŠ¥:
    - ì‹¤ì‹œê°„ ë°ì´í„° ë³€ê²½ ëª¨ë‹ˆí„°ë§
    - ì´ë²¤íŠ¸ ê¸°ë°˜ ì•Œë¦¼ ì‹œìŠ¤í…œ
    - ë°ì´í„° ë™ê¸°í™” ë° ë³µì œ
    """

    def __init__(self):
        self.change_streams: Dict[str, AsyncIOMotorChangeStream] = {}
        self.event_handlers: Dict[str, callable] = {}

    @safe_execution("start_realtime_metrics_stream")
    async def start_realtime_metrics_stream(self, event_handler: callable):
        """
        ë©”íŠ¸ë¦­ ì»¬ë ‰ì…˜ì˜ ì‹¤ì‹œê°„ ë³€ê²½ì‚¬í•­ì„ ìŠ¤íŠ¸ë¦¬ë°
        ë‹¨ì¼ MongoDB ì¸ìŠ¤í„´ìŠ¤ì—ì„œëŠ” í´ë§ ë°©ì‹ìœ¼ë¡œ ë™ì‘

        Args:
            event_handler: ë³€ê²½ì‚¬í•­ ì²˜ë¦¬ í•¨ìˆ˜
        """
        try:
            # Change Stream ì‹œë„ (replica setì—ì„œë§Œ ë™ì‘)
            try:
                pipeline = [
                    {
                        "$match": {
                            "operationType": {"$in": ["insert", "update"]},
                            "fullDocument.type": {"$exists": True},
                        }
                    }
                ]

                change_stream = mongodb_connection.metrics_collection.watch(
                    pipeline, full_document="updateLookup"
                )

                self.change_streams["metrics"] = change_stream
                self.event_handlers["metrics"] = event_handler
                logger.info("ğŸ“¡ ë©”íŠ¸ë¦­ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ (Change Stream)")

                # ë¹„ë™ê¸° ì´ë²¤íŠ¸ ì²˜ë¦¬ ë£¨í”„
                asyncio.create_task(self._process_change_events_loop("metrics"))

            except Exception as watch_error:
                # ë‹¨ì¼ MongoDB ì¸ìŠ¤í„´ìŠ¤ì—ì„œëŠ” change streamì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
                error_msg = str(watch_error)
                if "40573" in error_msg or "replica sets" in error_msg.lower():
                    logger.info("ğŸ“¡ ë‹¨ì¼ MongoDB ì¸ìŠ¤í„´ìŠ¤ ê°ì§€ - í´ë§ ë°©ì‹ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§ ëŒ€ì²´")
                else:
                    logger.warning(f"âš ï¸ Change Stream ì‚¬ìš© ë¶ˆê°€: {error_msg[:100]}...")
                logger.info("ğŸ“¡ í´ë§ ë°©ì‹ìœ¼ë¡œ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§ ì‹œì‘")

                # í´ë§ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´
                self.event_handlers["metrics"] = event_handler
                asyncio.create_task(self._polling_metrics_monitor("metrics"))

        except Exception as stream_error:
            logger.error(f"âŒ ë©”íŠ¸ë¦­ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì‹¤íŒ¨: {stream_error}")
            # ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•Šê³  ë¡œê·¸ë§Œ ë‚¨ê¹€ (ì„œë¹„ìŠ¤ ì‹œì‘ì´ ì¤‘ë‹¨ë˜ì§€ ì•Šë„ë¡)

    async def _process_change_events_loop(self, stream_name: str):
        """Change Stream ì´ë²¤íŠ¸ ì²˜ë¦¬ ë£¨í”„"""
        try:
            change_stream = self.change_streams[stream_name]
            event_handler = self.event_handlers[stream_name]

            async for change_event in change_stream:
                try:
                    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ í˜¸ì¶œ
                    await event_handler(change_event)

                except Exception as handler_error:
                    logger.error(f"âŒ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {handler_error}")

        except Exception as loop_error:
            error_msg = str(loop_error)
            if "40573" in error_msg or "replica sets" in error_msg.lower():
                logger.debug(f"ğŸ“¡ Change Stream ê¸°ëŠ¥ ë¹„í™œì„±í™”ë¨ (ë‹¨ì¼ MongoDB): {error_msg}")
            else:
                logger.error(f"âŒ ë³€ê²½ì‚¬í•­ ì²˜ë¦¬ ë£¨í”„ ì˜¤ë¥˜: {loop_error}")
        finally:
            # ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
            if stream_name in self.change_streams:
                await self.change_streams[stream_name].close()
                del self.change_streams[stream_name]

    async def _polling_metrics_monitor(self, stream_name: str):
        """í´ë§ ë°©ì‹ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§ (Change Stream ëŒ€ì²´)"""
        try:
            event_handler = self.event_handlers[stream_name]
            last_check_time = datetime.now()

            logger.info("ğŸ“¡ í´ë§ ë°©ì‹ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§ ì‹œì‘")

            while True:
                try:
                    # 30ì´ˆë§ˆë‹¤ í™•ì¸
                    await asyncio.sleep(30)

                    # ë§ˆì§€ë§‰ í™•ì¸ ì´í›„ ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ ì¡°íšŒ
                    new_metrics = await mongodb_connection.metrics_collection.find(
                        {
                            "timestamp": {"$gte": last_check_time},
                            "type": {"$exists": True},
                        }
                    ).to_list(100)

                    # ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ì´ ìˆìœ¼ë©´ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ í˜¸ì¶œ
                    for metric in new_metrics:
                        try:
                            # Change Stream ì´ë²¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜
                            mock_change_event = {
                                "operationType": "insert",
                                "fullDocument": metric,
                            }
                            await event_handler(mock_change_event)
                        except Exception as handler_error:
                            logger.error(
                                f"âŒ í´ë§ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {handler_error}"
                            )

                    # ë§ˆì§€ë§‰ í™•ì¸ ì‹œê°„ ì—…ë°ì´íŠ¸
                    last_check_time = datetime.now()

                except asyncio.CancelledError:
                    break
                except Exception as poll_error:
                    logger.error(f"âŒ í´ë§ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {poll_error}")
                    await asyncio.sleep(60)  # ì—ëŸ¬ ì‹œ 1ë¶„ ëŒ€ê¸°

        except Exception as monitor_error:
            logger.error(f"âŒ í´ë§ ëª¨ë‹ˆí„°ë§ ì‹œì‘ ì‹¤íŒ¨: {monitor_error}")
        finally:
            # ì •ë¦¬
            if stream_name in self.event_handlers:
                del self.event_handlers[stream_name]
            logger.info("ğŸ“¡ í´ë§ ë°©ì‹ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")

    async def stop_stream(self, stream_name: str):
        """íŠ¹ì • ìŠ¤íŠ¸ë¦¼ ì¤‘ì§€"""
        if stream_name in self.change_streams:
            await self.change_streams[stream_name].close()
            del self.change_streams[stream_name]
            if stream_name in self.event_handlers:
                del self.event_handlers[stream_name]
            logger.info(f"ğŸ“¡ ìŠ¤íŠ¸ë¦¼ ì¤‘ì§€: {stream_name}")


class MongoDBAutoManagement:
    """
    MongoDB ìë™ ë°ì´í„° ê´€ë¦¬ ë° ìµœì í™”

    ì£¼ìš” ê¸°ëŠ¥:
    - ì˜¤ë˜ëœ ë°ì´í„° ìë™ ì •ë¦¬
    - ì¸ë±ìŠ¤ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
    - ìë™ ë°±ì—… ë° ì•„ì¹´ì´ë¹™
    - ë°ì´í„° ì••ì¶• ë° ìµœì í™”
    """

    def __init__(self):
        self.cleanup_in_progress = False

    @safe_execution("auto_cleanup_old_data")
    async def auto_cleanup_old_data(self):
        """
        ì„¤ì •ëœ ë³´ê´€ ì •ì±…ì— ë”°ë¼ ì˜¤ë˜ëœ ë°ì´í„° ìë™ ì •ë¦¬
        """
        if self.cleanup_in_progress:
            logger.warning("âš ï¸ ë°ì´í„° ì •ë¦¬ ì‘ì—…ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return

        self.cleanup_in_progress = True
        try:
            with logger_manager.performance_logger("old_data_cleanup"):
                # ë™ì‹œ ì •ë¦¬ ì‘ì—… ì‹¤í–‰
                cleanup_results = await asyncio.gather(
                    self._cleanup_metrics_data(),
                    self._cleanup_cache_data(),
                    self._cleanup_temp_data(),
                    return_exceptions=True,
                )

                # ê²°ê³¼ ìš”ì•½
                total_deleted_docs = 0
                for result in cleanup_results:
                    if isinstance(result, dict) and "deleted_count" in result:
                        total_deleted_docs += result["deleted_count"]

                logger.info(f"ğŸ§¹ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: ì´ {total_deleted_docs}ê°œ ë¬¸ì„œ ì‚­ì œ")

        finally:
            self.cleanup_in_progress = False

    async def _cleanup_metrics_data(self) -> Dict[str, int]:
        """30ì¼ ì´ìƒ ëœ ë©”íŠ¸ë¦­ ë°ì´í„° ì •ë¦¬"""
        cutoff_date = datetime.now() - timedelta(days=30)

        delete_result = await mongodb_connection.metrics_collection.delete_many(
            {"timestamp": {"$lt": cutoff_date}}
        )

        logger.debug(f"ğŸ“Š ë©”íŠ¸ë¦­ ë°ì´í„° ì •ë¦¬: {delete_result.deleted_count}ê°œ ì‚­ì œ")
        return {"deleted_count": delete_result.deleted_count}

    async def _cleanup_cache_data(self) -> Dict[str, int]:
        """ë§Œë£Œëœ ìºì‹œ ë°ì´í„° ì •ë¦¬"""
        current_time = datetime.now()

        # ë§Œë£Œëœ ìŠ¤í‚¤ë§ˆ ìºì‹œ ì •ë¦¬
        schema_cutoff_time = current_time - timedelta(seconds=settings.schema_cache_ttl)
        schema_delete_result = (
            await mongodb_connection.schema_cache_collection.delete_many(
                {"created_at": {"$lt": schema_cutoff_time}}
            )
        )

        # ì˜¤ë˜ëœ ìŠ¤ë ˆë“œ ìºì‹œ ì •ë¦¬ (30ì¼)
        thread_cutoff_time = current_time - timedelta(days=30)
        thread_delete_result = (
            await mongodb_connection.thread_cache_collection.delete_many(
                {"last_used": {"$lt": thread_cutoff_time}}
            )
        )

        total_deleted = (
            schema_delete_result.deleted_count + thread_delete_result.deleted_count
        )
        logger.debug(f"ğŸ’¾ ìºì‹œ ë°ì´í„° ì •ë¦¬: {total_deleted}ê°œ ì‚­ì œ")
        return {"deleted_count": total_deleted}

    async def _cleanup_temp_data(self) -> Dict[str, int]:
        """ì„ì‹œ ë°ì´í„° ë° ë¡œê·¸ ì •ë¦¬"""
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        # ì‹¤ì œë¡œëŠ” ì„ì‹œ ì»¬ë ‰ì…˜ì´ë‚˜ ë¡œê·¸ ë°ì´í„° ì •ë¦¬ ë¡œì§ ì¶”ê°€
        return {"deleted_count": 0}

    @safe_execution("analyze_index_performance")
    async def analyze_index_performance(self) -> Dict[str, Any]:
        """
        ëª¨ë“  ì»¬ë ‰ì…˜ì˜ ì¸ë±ìŠ¤ ì„±ëŠ¥ ë¶„ì„

        Returns:
            Dict: ì¸ë±ìŠ¤ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼
        """
        try:
            collections = ["metrics", "schema_cache", "thread_cache"]
            analysis_result = {}

            for collection_name in collections:
                collection = getattr(
                    mongodb_connection,
                    f"{collection_name.replace('_', '_')}_collection",
                    None,
                )
                if collection:
                    # ì¸ë±ìŠ¤ í†µê³„ ì¡°íšŒ
                    index_stats = await collection.aggregate(
                        [{"$indexStats": {}}]
                    ).to_list(100)

                    analysis_result[collection_name] = {
                        "index_count": len(index_stats),
                        "index_details": index_stats,
                    }

            logger.info("ğŸ“ˆ ì¸ë±ìŠ¤ ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ")
            return analysis_result

        except Exception as analysis_error:
            raise DatabaseOperationException(
                "ì¸ë±ìŠ¤ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨", original_exception=analysis_error
            )

    @safe_execution("collect_database_statistics")
    async def collect_database_statistics(self) -> Dict[str, Any]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ í†µê³„ ìˆ˜ì§‘

        Returns:
            Dict: ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´
        """
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ì •ë³´
            db_stats = await mongodb_connection.main_database.command("dbStats")

            # ì»¬ë ‰ì…˜ë³„ í†µê³„
            ì»¬ë ‰ì…˜_í†µê³„ = {}
            ì»¬ë ‰ì…˜ë“¤ = await mongodb_connection.main_database.list_collection_names()

            for ì»¬ë ‰ì…˜ëª… in ì»¬ë ‰ì…˜ë“¤:
                try:
                    ì»¬ë ‰ì…˜ = mongodb_connection.main_database[ì»¬ë ‰ì…˜ëª…]
                    stats = await ì»¬ë ‰ì…˜.aggregate(
                        [
                            {
                                "$group": {
                                    "_id": None,
                                    "ë¬¸ì„œ_ìˆ˜": {"$sum": 1},
                                    "í‰ê· _ë¬¸ì„œ_í¬ê¸°": {"$avg": {"$bsonSize": "$$ROOT"}},
                                    "ì´_í¬ê¸°": {"$sum": {"$bsonSize": "$$ROOT"}},
                                }
                            }
                        ]
                    ).to_list(1)

                    if stats:
                        ì»¬ë ‰ì…˜_í†µê³„[ì»¬ë ‰ì…˜ëª…] = stats[0]

                except Exception:
                    # íŠ¹ì • ì»¬ë ‰ì…˜ ë¶„ì„ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
                    continue

            return {
                "ë°ì´í„°ë² ì´ìŠ¤_í†µê³„": {
                    "ì´ë¦„": db_stats.get("db"),
                    "ì»¬ë ‰ì…˜_ìˆ˜": db_stats.get("collections"),
                    "ì¸ë±ìŠ¤_ìˆ˜": db_stats.get("indexes"),
                    "ë°ì´í„°_í¬ê¸°_MB": round(
                        db_stats.get("dataSize", 0) / (1024 * 1024), 2
                    ),
                    "ì¸ë±ìŠ¤_í¬ê¸°_MB": round(
                        db_stats.get("indexSize", 0) / (1024 * 1024), 2
                    ),
                    "ì´_í¬ê¸°_MB": round(
                        db_stats.get("storageSize", 0) / (1024 * 1024), 2
                    ),
                },
                "ì»¬ë ‰ì…˜ë³„_í†µê³„": ì»¬ë ‰ì…˜_í†µê³„,
                "ìˆ˜ì§‘_ì‹œê°„": datetime.now().isoformat(),
            }

        except Exception as stats_error:
            raise DatabaseOperationException(
                "ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨", original_exception=stats_error
            )


class MongoDBBackupRestore:
    """
    MongoDB ë°±ì—… ë° ë³µêµ¬ ê´€ë¦¬

    ì£¼ìš” ê¸°ëŠ¥:
    - ìë™ ì •ê¸° ë°±ì—…
    - ì„ íƒì  ë°ì´í„° ë°±ì—…
    - ë°±ì—… íŒŒì¼ ê´€ë¦¬
    - ë³µêµ¬ ì‘ì—… ì§€ì›
    """

    def __init__(self):
        self.ë°±ì—…_ì§„í–‰ì¤‘ = False

    @safe_execution("ì¤‘ìš”_ë°ì´í„°_ë°±ì—…")
    async def ì¤‘ìš”_ë°ì´í„°_ë°±ì—…(self) -> Dict[str, Any]:
        """
        ì¤‘ìš” ë°ì´í„°ì˜ ë°±ì—… ìƒì„±

        Returns:
            Dict: ë°±ì—… ê²°ê³¼ ì •ë³´
        """
        if self.ë°±ì—…_ì§„í–‰ì¤‘:
            return {"ìƒíƒœ": "ì´ë¯¸_ì§„í–‰ì¤‘", "ë©”ì‹œì§€": "ë°±ì—…ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤"}

        self.ë°±ì—…_ì§„í–‰ì¤‘ = True
        try:
            with logger_manager.performance_logger("ì¤‘ìš”_ë°ì´í„°_ë°±ì—…"):
                ë°±ì—…_ê²°ê³¼ = {"ë°±ì—…_ì‹œê°„": datetime.now().isoformat(), "ë°±ì—…_í•­ëª©ë“¤": []}

                # ìŠ¤í‚¤ë§ˆ ìºì‹œ ë°±ì—…
                ìŠ¤í‚¤ë§ˆ_ë¬¸ì„œë“¤ = await mongodb_connection.schema_cache_collection.find(
                    {}
                ).to_list(1000)
                ë°±ì—…_ê²°ê³¼["ë°±ì—…_í•­ëª©ë“¤"].append(
                    {
                        "íƒ€ì…": "ìŠ¤í‚¤ë§ˆ_ìºì‹œ",
                        "ë¬¸ì„œ_ìˆ˜": len(ìŠ¤í‚¤ë§ˆ_ë¬¸ì„œë“¤),
                        "í¬ê¸°_ì¶”ì •_KB": sum(len(str(doc)) for doc in ìŠ¤í‚¤ë§ˆ_ë¬¸ì„œë“¤)
                        // 1024,
                    }
                )

                # ìµœê·¼ 7ì¼ ë©”íŠ¸ë¦­ ë°±ì—…
                ì¼ì£¼ì¼_ì „ = datetime.now() - timedelta(days=7)
                ë©”íŠ¸ë¦­_ë¬¸ì„œë“¤ = await mongodb_connection.metrics_collection.find(
                    {"timestamp": {"$gte": ì¼ì£¼ì¼_ì „}}
                ).to_list(10000)
                ë°±ì—…_ê²°ê³¼["ë°±ì—…_í•­ëª©ë“¤"].append(
                    {
                        "íƒ€ì…": "ìµœê·¼_ë©”íŠ¸ë¦­",
                        "ë¬¸ì„œ_ìˆ˜": len(ë©”íŠ¸ë¦­_ë¬¸ì„œë“¤),
                        "í¬ê¸°_ì¶”ì •_KB": sum(len(str(doc)) for doc in ë©”íŠ¸ë¦­_ë¬¸ì„œë“¤)
                        // 1024,
                    }
                )

                # í™œì„± ìŠ¤ë ˆë“œ ì •ë³´ ë°±ì—…
                í™œì„±_ìŠ¤ë ˆë“œë“¤ = await mongodb_connection.thread_cache_collection.find(
                    {"last_used": {"$gte": datetime.now() - timedelta(days=7)}}
                ).to_list(1000)
                ë°±ì—…_ê²°ê³¼["ë°±ì—…_í•­ëª©ë“¤"].append(
                    {
                        "íƒ€ì…": "í™œì„±_ìŠ¤ë ˆë“œ",
                        "ë¬¸ì„œ_ìˆ˜": len(í™œì„±_ìŠ¤ë ˆë“œë“¤),
                        "í¬ê¸°_ì¶”ì •_KB": sum(len(str(doc)) for doc in í™œì„±_ìŠ¤ë ˆë“œë“¤)
                        // 1024,
                    }
                )

                # ì‹¤ì œ ë°±ì—… ì €ì¥ì€ íŒŒì¼ ì‹œìŠ¤í…œì´ë‚˜ í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ì— êµ¬í˜„
                # ì—¬ê¸°ì„œëŠ” ë©”íƒ€ë°ì´í„°ë§Œ ê¸°ë¡

                logger.info(
                    f"ğŸ’¾ ë°±ì—… ì™„ë£Œ: ì´ {sum(item['ë¬¸ì„œ_ìˆ˜'] for item in ë°±ì—…_ê²°ê³¼['ë°±ì—…_í•­ëª©ë“¤'])}ê°œ ë¬¸ì„œ"
                )
                return ë°±ì—…_ê²°ê³¼

        finally:
            self.ë°±ì—…_ì§„í–‰ì¤‘ = False


# Global instances (lazy initialization)
mongodb_analysis_service = None
mongodb_realtime_streaming = None
mongodb_auto_management = None
mongodb_backup_restore = None


def get_mongodb_analysis_service():
    """Get MongoDB analysis service instance with lazy initialization"""
    global mongodb_analysis_service
    if mongodb_analysis_service is None:
        mongodb_analysis_service = MongoDBAnalysisService()
    return mongodb_analysis_service


def get_mongodb_realtime_streaming():
    """Get MongoDB real-time streaming service with lazy initialization"""
    global mongodb_realtime_streaming
    if mongodb_realtime_streaming is None:
        mongodb_realtime_streaming = MongoDBRealTimeStreaming()
    return mongodb_realtime_streaming


def get_mongodb_auto_management():
    """Get MongoDB auto management service with lazy initialization"""
    global mongodb_auto_management
    if mongodb_auto_management is None:
        mongodb_auto_management = MongoDBAutoManagement()
    return mongodb_auto_management


def get_mongodb_backup_restore():
    """Get MongoDB backup restore service with lazy initialization"""
    global mongodb_backup_restore
    if mongodb_backup_restore is None:
        mongodb_backup_restore = MongoDBBackupRestore()
    return mongodb_backup_restore


# ìŠ¤ì¼€ì¤„ë§ì„ ìœ„í•œ ìë™ ì‘ì—… í•¨ìˆ˜ë“¤
async def daily_auto_cleanup_task():
    """ë§¤ì¼ ìë™ ì‹¤í–‰ë  ë°ì´í„° ì •ë¦¬ ì‘ì—…"""
    try:
        mongodb_auto_management = get_mongodb_auto_management()
        await mongodb_auto_management.auto_cleanup_old_data()
        logger.info("âœ… ì¼ì¼ ìë™ ì •ë¦¬ ì‘ì—… ì™„ë£Œ")
    except Exception as cleanup_error:
        logger.error(f"âŒ ì¼ì¼ ìë™ ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨: {cleanup_error}")


async def weekly_backup_task():
    """ì£¼ê°„ ìë™ ë°±ì—… ì‘ì—…"""
    try:
        mongodb_backup_restore = get_mongodb_backup_restore()
        await mongodb_backup_restore.backup_critical_data()
        logger.info("âœ… ì£¼ê°„ ë°±ì—… ì‘ì—… ì™„ë£Œ")
    except Exception as backup_error:
        logger.error(f"âŒ ì£¼ê°„ ë°±ì—… ì‘ì—… ì‹¤íŒ¨: {backup_error}")


async def start_realtime_performance_monitoring():
    """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""

    async def performance_event_handler(change_event):
        """ì„±ëŠ¥ ê´€ë ¨ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            document = change_event.get("fullDocument", {})
            if document.get("type") == "command_usage":
                execution_time = document.get("execution_time", 0)

                # ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ì‹¤í–‰ ì‹œê°„ ê°ì§€ (5ì´ˆ ì´ˆê³¼)
                if execution_time > 5.0:
                    logger.warning(
                        f"ğŸŒ ëŠë¦° ëª…ë ¹ì–´ ê°ì§€: {document.get('command')} "
                        f"ì‹¤í–‰ì‹œê°„ {execution_time:.2f}ì´ˆ"
                    )
        except Exception as handler_error:
            logger.error(f"ì„±ëŠ¥ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì˜¤ë¥˜: {handler_error}")

    try:
        mongodb_realtime_streaming = get_mongodb_realtime_streaming()
        await mongodb_realtime_streaming.start_realtime_metrics_stream(
            performance_event_handler
        )
        logger.info("ğŸ“¡ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    except Exception as monitoring_error:
        logger.error(f"âŒ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘ ì‹¤íŒ¨: {monitoring_error}")
