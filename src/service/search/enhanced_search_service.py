"""
ê³ ì„±ëŠ¥ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ëª¨ë“ˆ
- MongoDB í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ í™œìš©
- ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰
- ì„±ëŠ¥ ìµœì í™”
"""

import asyncio
import re
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

from src.core.database import get_meetup_collection, mongodb_connection
from src.core.logger import get_logger
from src.core.exceptions import safe_execution

# Module logger
logger = get_logger("services.enhanced_search")


class EnhancedSearchService:
    """ê³ ì„±ëŠ¥ ê²€ìƒ‰ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.cache = {}  # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ
        self.cache_ttl = 300  # 5ë¶„ ìºì‹œ TTL

    @safe_execution("search_pages_enhanced")
    async def search_pages_enhanced(
        self,
        query: str,
        page_type: str = None,
        user_filter: str = None,
        days_limit: int = 90,
        limit: int = 20,
    ) -> Dict[str, Any]:
        """
        ê³ ì„±ëŠ¥ í˜ì´ì§€ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ í‚¤ì›Œë“œ
            page_type: í˜ì´ì§€ íƒ€ì… í•„í„°
            user_filter: ì‚¬ìš©ì ID í•„í„°
            days_limit: ê²€ìƒ‰ ê¸°ê°„ ì œí•œ (ì¼)
            limit: ê²°ê³¼ ê°œìˆ˜ ì œí•œ
        """
        if not query or len(query.strip()) < 2:
            return {
                "total_results": 0,
                "results": [],
                "query": query,
                "filters": {"type": page_type, "user": user_filter, "days": days_limit},
                "search_time_ms": 0,
            }

        start_time = datetime.now()

        try:
            await mongodb_connection.connect_database()
            collection = get_meetup_collection("notion_pages")

            # 1. MongoDB í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ê°€ì¥ ë¹ ë¦„)
            text_results = await self._text_search(collection, query, limit)

            # 2. ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ (ì œëª© ìš°ì„ )
            weighted_results = await self._weighted_search(
                collection, query, page_type, user_filter, days_limit, limit
            )

            # 3. ê²°ê³¼ í•©ì¹˜ê¸° ë° ì¤‘ë³µ ì œê±°
            all_results = self._merge_results(text_results, weighted_results)

            # 4. ìµœì¢… ê²°ê³¼ ì •ë ¬ ë° ì œí•œ
            final_results = self._rank_and_limit_results(all_results, query, limit)

            search_time = (datetime.now() - start_time).total_seconds() * 1000

            logger.info(
                f"ğŸ” ê³ ì„±ëŠ¥ ê²€ìƒ‰ ì™„ë£Œ: '{query}' -> {len(final_results)}ê°œ ê²°ê³¼ ({search_time:.1f}ms)"
            )

            return {
                "total_results": len(final_results),
                "results": final_results,
                "query": query,
                "filters": {"type": page_type, "user": user_filter, "days": days_limit},
                "search_time_ms": search_time,
            }

        except Exception as e:
            logger.error(f"âŒ ê³ ì„±ëŠ¥ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise e
        finally:
            await mongodb_connection.disconnect()

    @safe_execution("text_search")
    async def _text_search(
        self, collection, query: str, limit: int
    ) -> List[Dict[str, Any]]:
        """MongoDB í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ê²€ìƒ‰ (ê°€ì¥ ë¹ ë¦„)"""
        try:
            # MongoDB í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‚¬ìš©
            cursor = (
                collection.find(
                    {"$text": {"$search": query}}, {"score": {"$meta": "textScore"}}
                )
                .sort([("score", {"$meta": "textScore"})])
                .limit(limit)
            )

            results = await cursor.to_list(None)

            # ê²€ìƒ‰ ì ìˆ˜ ì¶”ê°€
            for result in results:
                result["search_score"] = result.get("score", 0)
                result["search_type"] = "text_index"

            logger.debug(f"ğŸ“Š í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ê²€ìƒ‰: {len(results)}ê°œ ê²°ê³¼")
            return results

        except Exception as e:
            logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    @safe_execution("weighted_search")
    async def _weighted_search(
        self,
        collection,
        query: str,
        page_type: str,
        user_filter: str,
        days_limit: int,
        limit: int,
    ) -> List[Dict[str, Any]]:
        """ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ (ì œëª© > ë‚´ìš©)"""

        # ê²€ìƒ‰ ì¡°ê±´ êµ¬ì„±
        search_conditions = []

        # ë‚ ì§œ ë²”ìœ„ í•„í„°
        if days_limit:
            since_date = datetime.now() - timedelta(days=days_limit)
            search_conditions.append({"created_at": {"$gte": since_date}})

        # íƒ€ì… í•„í„°
        if page_type and page_type != "all":
            search_conditions.append({"page_type": page_type})

        # ì‚¬ìš©ì í•„í„°
        if user_filter and user_filter != "all":
            search_conditions.append({"created_by": user_filter})

        # MongoDB ì¿¼ë¦¬ êµ¬ì„±
        if search_conditions:
            mongo_query = {"$and": search_conditions}
        else:
            mongo_query = {}

        # ëª¨ë“  í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (Python ë ˆë²¨ì—ì„œ ê°€ì¤‘ì¹˜ ê³„ì‚°)
        all_pages = await collection.find(mongo_query).to_list(None)

        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰
        weighted_results = []
        query_lower = query.lower()

        for page in all_pages:
            score = 0
            search_type = "none"

            title = page.get("title", "")
            content = page.get("content", "")
            search_text = page.get("search_text", "")

            # ì œëª© ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
            if query_lower in title.lower():
                title_matches = len(re.findall(re.escape(query_lower), title.lower()))
                score += title_matches * 10  # ì œëª© ê°€ì¤‘ì¹˜ 10
                search_type = "title"

            # ë‚´ìš© ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ ì¤‘ê°„)
            if content and query_lower in content.lower():
                content_matches = len(
                    re.findall(re.escape(query_lower), content.lower())
                )
                score += content_matches * 3  # ë‚´ìš© ê°€ì¤‘ì¹˜ 3
                if search_type == "title":
                    search_type = "title_content"
                else:
                    search_type = "content"

            # í†µí•© ê²€ìƒ‰ í…ìŠ¤íŠ¸ (ê°€ì¤‘ì¹˜ ë‚®ìŒ)
            if search_text and query_lower in search_text.lower():
                search_matches = len(
                    re.findall(re.escape(query_lower), search_text.lower())
                )
                score += search_matches * 1  # í†µí•© ê²€ìƒ‰ ê°€ì¤‘ì¹˜ 1
                if search_type == "none":
                    search_type = "search_text"

            # ì ìˆ˜ê°€ ìˆëŠ” ê²°ê³¼ë§Œ ì¶”ê°€
            if score > 0:
                page["search_score"] = score
                page["search_type"] = search_type
                weighted_results.append(page)

        # ì ìˆ˜ìˆœ ì •ë ¬
        weighted_results.sort(key=lambda x: x.get("search_score", 0), reverse=True)

        logger.debug(f"ğŸ“Š ê°€ì¤‘ì¹˜ ê²€ìƒ‰: {len(weighted_results)}ê°œ ê²°ê³¼")
        return weighted_results[:limit]

    def _merge_results(
        self, text_results: List[Dict], weighted_results: List[Dict]
    ) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ í•©ì¹˜ê¸° ë° ì¤‘ë³µ ì œê±°"""
        seen_page_ids = set()
        merged_results = []

        # í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ê²°ê³¼ ìš°ì„  (MongoDBê°€ ê³„ì‚°í•œ ì ìˆ˜)
        for result in text_results:
            page_id = result.get("page_id")
            if page_id not in seen_page_ids:
                seen_page_ids.add(page_id)
                merged_results.append(result)

        # ê°€ì¤‘ì¹˜ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ì¤‘ë³µ ì œì™¸)
        for result in weighted_results:
            page_id = result.get("page_id")
            if page_id not in seen_page_ids:
                seen_page_ids.add(page_id)
                merged_results.append(result)

        return merged_results

    def _rank_and_limit_results(
        self, results: List[Dict], query: str, limit: int
    ) -> List[Dict]:
        """ê²°ê³¼ ì •ë ¬ ë° ì œí•œ"""
        # ê²€ìƒ‰ ì ìˆ˜ìˆœ ì •ë ¬
        results.sort(key=lambda x: x.get("search_score", 0), reverse=True)

        # ê²°ê³¼ ì œí•œ
        final_results = results[:limit]

        # ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        for result in final_results:
            result["search_context"] = self._extract_search_context(
                result.get("content", ""), query
            )

        return final_results

    def _extract_search_context(
        self, content: str, query: str, context_length: int = 150
    ) -> str:
        """ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if not content:
            return ""

        query_lower = query.lower()
        content_lower = content.lower()

        # ê²€ìƒ‰ì–´ ìœ„ì¹˜ ì°¾ê¸°
        query_pos = content_lower.find(query_lower)
        if query_pos == -1:
            return (
                content[:context_length] + "..."
                if len(content) > context_length
                else content
            )

        # ê²€ìƒ‰ì–´ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        start = max(0, query_pos - context_length // 2)
        end = min(len(content), query_pos + context_length // 2)

        context = content[start:end]
        if start > 0:
            context = "..." + context
        if end < len(content):
            context = context + "..."

        return context

    @safe_execution("get_search_suggestions_enhanced")
    async def get_search_suggestions_enhanced(self, query: str) -> Dict[str, Any]:
        """í–¥ìƒëœ ê²€ìƒ‰ ì œì•ˆ"""
        try:
            await mongodb_connection.connect_database()
            collection = get_meetup_collection("notion_pages")

            suggestions = {
                "did_you_mean": [],
                "related_keywords": [],
                "popular_searches": [],
                "recent_activity": [],
            }

            # 1. ì—°ê´€ ê²€ìƒ‰ì–´ (í‚¤ì›Œë“œ ê¸°ë°˜)
            if len(query) > 2:
                # ìµœê·¼ í˜ì´ì§€ë“¤ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                recent_pages = (
                    await collection.find({})
                    .sort("created_at", -1)
                    .limit(20)
                    .to_list(None)
                )

                keywords = set()
                for page in recent_pages:
                    title = page.get("title", "")
                    content = page.get("content", "")

                    # ì œëª©ê³¼ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                    words = re.findall(r"\b\w{2,}\b", f"{title} {content}")
                    for word in words:
                        if word.lower() != query.lower() and len(word) >= 2:
                            keywords.add(word)

                # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
                keyword_freq = {}
                for keyword in keywords:
                    count = await collection.count_documents(
                        {
                            "$or": [
                                {"title": {"$regex": keyword, "$options": "i"}},
                                {"content": {"$regex": keyword, "$options": "i"}},
                            ]
                        }
                    )
                    keyword_freq[keyword] = count

                # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
                suggestions["related_keywords"] = sorted(
                    keyword_freq.items(), key=lambda x: x[1], reverse=True
                )[:5]

            # 2. ì¸ê¸° ê²€ìƒ‰ì–´
            recent_pages = (
                await collection.find({}).sort("created_at", -1).limit(50).to_list(None)
            )
            word_counts = {}

            for page in recent_pages:
                title = page.get("title", "")
                words = re.findall(r"\b\w{2,}\b", title)
                for word in words:
                    word_counts[word] = word_counts.get(word, 0) + 1

            suggestions["popular_searches"] = sorted(
                word_counts.items(), key=lambda x: x[1], reverse=True
            )[:5]

            # 3. ìµœê·¼ í™œë™
            suggestions["recent_activity"] = [
                {
                    "title": page.get("title", ""),
                    "type": page.get("page_type", "unknown"),
                    "created_at": page.get("created_at"),
                }
                for page in recent_pages[:5]
            ]

            return suggestions

        except Exception as e:
            logger.warning(f"âš ï¸ ê²€ìƒ‰ ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "did_you_mean": [],
                "related_keywords": [],
                "popular_searches": [],
                "recent_activity": [],
            }
        finally:
            await mongodb_connection.disconnect()

    def format_search_results_enhanced(self, search_data: Dict[str, Any]) -> str:
        """í–¥ìƒëœ ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…"""
        query = search_data.get("query", "")
        total = search_data.get("total_results", 0)
        results = search_data.get("results", [])
        filters = search_data.get("filters", {})
        search_time = search_data.get("search_time_ms", 0)
        suggestions = search_data.get("suggestions", {})

        if total == 0:
            msg = f"ğŸ” **ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ**\n"
            msg += f"ê²€ìƒ‰ì–´: `{query}`\n\n"

            if suggestions:
                if suggestions.get("related_keywords"):
                    keywords = [kw for kw, _ in suggestions["related_keywords"][:3]]
                    msg += f"ğŸ’¡ **ì—°ê´€ ê²€ìƒ‰ì–´**: {', '.join(keywords)}\n\n"

                if suggestions.get("popular_searches"):
                    popular = [kw for kw, _ in suggestions["popular_searches"][:3]]
                    msg += f"ğŸ”¥ **ì¸ê¸° ê²€ìƒ‰ì–´**: {', '.join(popular)}\n\n"
            else:
                msg += "ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”."

            return msg

        msg = f"ğŸ” **ê²€ìƒ‰ ê²°ê³¼** (`{query}`)\n"
        msg += f"ğŸ“Š ì´ {total}ê°œ ê²°ê³¼"

        if search_time > 0:
            msg += f" ({search_time:.1f}ms)"

        # í•„í„° ì •ë³´ í‘œì‹œ
        filter_info = []
        if filters.get("type") and filters["type"] != "all":
            filter_info.append(f"íƒ€ì…: {filters['type']}")
        if filters.get("user") and filters["user"] != "all":
            filter_info.append(f"ì‚¬ìš©ì: {filters['user'][-4:]}")
        if filters.get("days"):
            filter_info.append(f"ìµœê·¼ {filters['days']}ì¼")

        if filter_info:
            msg += f" ({', '.join(filter_info)})"

        msg += "\n\n"

        # ê²°ê³¼ ëª©ë¡
        for i, result in enumerate(results[:10], 1):
            title = result.get("title", "ì œëª© ì—†ìŒ")
            page_type = result.get("page_type", "unknown")
            created_at = result.get("created_at")
            user_id = result.get("created_by", "unknown")
            search_type = result.get("search_type", "unknown")
            search_score = result.get("search_score", 0)

            # íƒ€ì… ì´ëª¨ì§€
            type_emoji = {
                "meeting": "ğŸ“…",
                "task": "âœ…",
                "document": "ğŸ“„",
                "note": "ğŸ“",
            }.get(page_type, "ğŸ“„")

            # ë‚ ì§œ í¬ë§·
            date_str = "ë‚ ì§œ ë¯¸ìƒ"
            if created_at:
                if isinstance(created_at, datetime):
                    date_str = created_at.strftime("%m/%d %H:%M")
                else:
                    date_str = str(created_at)[:10]

            msg += f"{type_emoji} **{title}**\n"
            msg += f"   ğŸ“… {date_str} | ğŸ‘¤ User {user_id[-4:]} | ğŸ“‚ {page_type}"

            # ê²€ìƒ‰ íƒ€ì… í‘œì‹œ
            if search_type != "unknown":
                msg += f" | ğŸ” {search_type}"

            msg += "\n"

            # ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ í‘œì‹œ
            if result.get("search_context"):
                context = result["search_context"][:120]
                msg += f"   ğŸ’¬ {context}\n"

            msg += "\n"

        # ë” ë§ì€ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
        if total > 10:
            msg += f"... ê·¸ ì™¸ {total - 10}ê°œ ê²°ê³¼ ë” ìˆìŒ\n\n"

        return msg


# Global enhanced search service instance
enhanced_search_service = EnhancedSearchService()
