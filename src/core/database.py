"""
MongoDB ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ìš´ì˜ ê´€ë¦¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ MongoDBì™€ì˜ ëª¨ë“  ìƒí˜¸ì‘ìš©ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤:
- ë¹„ë™ê¸° ì—°ê²° ê´€ë¦¬ (Motor ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
- ìŠ¤í‚¤ë§ˆ ìºì‹± ì‹œìŠ¤í…œ (API í˜¸ì¶œ ìµœì†Œí™”)
- ë””ìŠ¤ì½”ë“œ ìŠ¤ë ˆë“œ ì •ë³´ ìºì‹±
- ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì €ì¥
- ì¸ë±ìŠ¤ ìë™ ìƒì„± ë° ê´€ë¦¬
"""

from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
from motor.motor_asyncio import (
    AsyncIOMotorClient,
    AsyncIOMotorDatabase,
    AsyncIOMotorCollection,
)

from .config import settings
from .logger import get_logger
from .exceptions import DatabaseConnectionException, DatabaseOperationException

# Module logger
logger = get_logger("database")


class MongoDBConnectionManager:
    """
    MongoDB ì—°ê²° ë° ìƒëª…ì£¼ê¸° ê´€ë¦¬ í´ë˜ìŠ¤

    ì£¼ìš” ê¸°ëŠ¥:
    - ë¹„ë™ê¸° MongoDB ì—°ê²° ì„¤ì • ë° í•´ì œ
    - ì—°ê²° ìƒíƒœ ëª¨ë‹ˆí„°ë§ ë° ìë™ ì¬ì—°ê²°
    - ì¸ë±ìŠ¤ ìë™ ìƒì„±ìœ¼ë¡œ ì¿¼ë¦¬ ì„±ëŠ¥ ìµœì í™”
    - ì»¬ë ‰ì…˜ë³„ ì ‘ê·¼ ë©”ì„œë“œ ì œê³µ
    """

    def __init__(self):
        # Connection-related instance variables
        self.mongo_client: Optional[AsyncIOMotorClient] = None
        self.main_database: Optional[AsyncIOMotorDatabase] = None
        self.connection_status = False

    async def connect_database(self):
        """
        MongoDBì— ë¹„ë™ê¸° ì—°ê²°ì„ ìˆ˜í–‰í•˜ê³  í•„ìš”í•œ ì´ˆê¸° ì„¤ì • ì‹¤í–‰

        ì—°ê²° ê³¼ì •:
        1. MongoDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (Motor ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
        2. ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        3. ping ëª…ë ¹ìœ¼ë¡œ ì—°ê²° í…ŒìŠ¤íŠ¸
        4. ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±

        Raises:
            DB_ì—°ê²°_ì˜ˆì™¸: MongoDB ì—°ê²° ì‹¤íŒ¨ ì‹œ
        """
        try:
            # Motor í´ë¼ì´ì–¸íŠ¸ë¡œ ë¹„ë™ê¸° MongoDB ì—°ê²° ìƒì„±
            self.mongo_client = AsyncIOMotorClient(settings.mongodb_url)
            self.main_database = self.mongo_client[settings.mongodb_db_name]

            # ì—°ê²° í…ŒìŠ¤íŠ¸: admin ë°ì´í„°ë² ì´ìŠ¤ì— ping ëª…ë ¹ ì „ì†¡
            await self.mongo_client.admin.command("ping")
            self.connection_status = True
            logger.info(f"âœ… MongoDB ì—°ê²° ì„±ê³µ: {settings.mongodb_url}")

            # ì¿¼ë¦¬ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
            await self._create_required_indexes()

        except Exception as connection_error:
            logger.error(f"âŒ MongoDB ì—°ê²° ì‹¤íŒ¨: {connection_error}")
            raise DatabaseConnectionException(
                f"MongoDB ì—°ê²° ë¶ˆê°€: {settings.mongodb_url}",
                original_exception=connection_error,
                details={"mongodb_url": settings.mongodb_url},
            )

    async def disconnect(self):
        """
        MongoDB ì—°ê²°ì„ ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•˜ê³  ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        """
        if self.mongo_client:
            await self.mongo_client.close()
            self.connection_status = False
            logger.info("ğŸ”Œ MongoDB ì—°ê²° ì¢…ë£Œ ì™„ë£Œ")

    async def _create_required_indexes(self):
        """
        ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ í•µì‹¬ ì¸ë±ìŠ¤ë“¤ì„ ìë™ ìƒì„±

        ìƒì„±ë˜ëŠ” ì¸ë±ìŠ¤ë“¤:
        - schema_cache: db_id (ê³ ìœ í‚¤), created_at (TTLìš©)
        - thread_cache: (channel_id, thread_name) ë³µí•© ê³ ìœ í‚¤, created_at
        - metrics: timestamp (ì‹œê³„ì—´ ë°ì´í„° ì •ë ¬ìš©)
        """
        if self.main_database is None:
            logger.warning("âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ì–´ ì¸ë±ìŠ¤ ìƒì„± ê±´ë„ˆëœ€")
            return

        try:
            # 1. ìŠ¤í‚¤ë§ˆ ìºì‹œ ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤
            schema_cache_collection = self.main_database.schema_cache
            await schema_cache_collection.create_index(
                "db_id", unique=True
            )  # ë…¸ì…˜ DB ID ê³ ìœ í‚¤
            await schema_cache_collection.create_index(
                "created_at"
            )  # ìºì‹œ ë§Œë£Œ ì‹œê°„ í™•ì¸ìš©

            # 2. ìŠ¤ë ˆë“œ ìºì‹œ ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤
            thread_cache_collection = self.main_database.thread_cache
            # ì±„ë„IDì™€ ìŠ¤ë ˆë“œëª… ì¡°í•©ìœ¼ë¡œ ê³ ìœ  ìŠ¤ë ˆë“œ ì‹ë³„
            await thread_cache_collection.create_index(
                [("channel_id", 1), ("thread_name", 1)], unique=True
            )
            await thread_cache_collection.create_index("created_at")

            # 3. ë©”íŠ¸ë¦­ ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤
            metrics_collection = self.main_database.metrics
            await metrics_collection.create_index("timestamp")  # ì‹œê°„ìˆœ ì •ë ¬ìš©
            await metrics_collection.create_index("type")  # ë©”íŠ¸ë¦­ íƒ€ì…ë³„ í•„í„°ë§ìš©

            # 4. Notion í˜ì´ì§€ ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤ (ì„±ëŠ¥ ìµœì í™”)
            notion_pages_collection = self.main_database.notion_pages
            await notion_pages_collection.create_index(
                "page_id", unique=True
            )  # í˜ì´ì§€ ID ê³ ìœ í‚¤
            await notion_pages_collection.create_index("database_id")  # DBë³„ í•„í„°ë§
            await notion_pages_collection.create_index(
                "page_type"
            )  # í˜ì´ì§€ íƒ€ì…ë³„ í•„í„°ë§
            await notion_pages_collection.create_index(
                "last_edited_time"
            )  # ìˆ˜ì • ì‹œê°„ìˆœ ì •ë ¬
            await notion_pages_collection.create_index("created_by")  # ìƒì„±ìë³„ í•„í„°ë§
            await notion_pages_collection.create_index(
                [("title", "text"), ("content", "text")]
            )  # í…ìŠ¤íŠ¸ ê²€ìƒ‰ìš©
            await notion_pages_collection.create_index(
                "last_synced"
            )  # ë™ê¸°í™” ì‹œê°„ìˆœ ì •ë ¬

            # 5. í˜ì´ì§€ ë‚´ìš© ìºì‹œ ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤ (ì„±ëŠ¥ ìµœì í™”)
            page_cache_collection = self.main_database.page_content_cache
            await page_cache_collection.create_index(
                "page_id", unique=True
            )  # í˜ì´ì§€ ID ê³ ìœ í‚¤
            await page_cache_collection.create_index(
                "cached_at", expireAfterSeconds=3600
            )  # TTL 1ì‹œê°„

            # 6. ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ë³µí•© ì¸ë±ìŠ¤
            await notion_pages_collection.create_index(
                [("page_type", 1), ("created_by", 1), ("last_edited_time", -1)]
            )  # ë³µí•© ê²€ìƒ‰ìš©
            await notion_pages_collection.create_index(
                [("database_id", 1), ("last_synced", -1)]
            )  # ë™ê¸°í™” ìµœì í™”ìš©

            # ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ (ë¡œê·¸ ì œê±°)

        except Exception as index_error:
            logger.error(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {index_error}")
            # ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•ŠìŒ

    @property
    def schema_cache_collection(self) -> AsyncIOMotorCollection:
        """
        ë…¸ì…˜ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ìºì‹±í•˜ëŠ” ì»¬ë ‰ì…˜ ë°˜í™˜

        ìš©ë„: API í˜¸ì¶œ íšŸìˆ˜ ìµœì†Œí™”ë¥¼ ìœ„í•´ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì„ì‹œ ì €ì¥
        TTL: 1ì‹œê°„ (settings.schema_cache_ttl)
        """
        return self.main_database.schema_cache

    @property
    def thread_cache_collection(self) -> AsyncIOMotorCollection:
        """
        ë””ìŠ¤ì½”ë“œ ìŠ¤ë ˆë“œ ì •ë³´ë¥¼ ìºì‹±í•˜ëŠ” ì»¬ë ‰ì…˜ ë°˜í™˜

        ìš©ë„: ì¼ì¼ ìŠ¤ë ˆë“œ ìƒì„±/ì¡°íšŒ ì„±ëŠ¥ í–¥ìƒ
        í‚¤: (channel_id, thread_name) ì¡°í•©
        """
        return self.main_database.thread_cache

    @property
    def metrics_collection(self) -> AsyncIOMotorCollection:
        """
        ì„±ëŠ¥ ë° ì‚¬ìš©ëŸ‰ ë©”íŠ¸ë¦­ì„ ì €ì¥í•˜ëŠ” ì»¬ë ‰ì…˜ ë°˜í™˜

        ìš©ë„: ëª…ë ¹ì–´ ì‚¬ìš© í†µê³„, ì—ëŸ¬ ë°œìƒ ë¹ˆë„, ì‘ë‹µ ì‹œê°„ ë“± ì¶”ì 
        """
        return self.main_database.metrics

    def get_collection(self, collection_name: str) -> AsyncIOMotorCollection:
        """
        ë™ì ìœ¼ë¡œ ì»¬ë ‰ì…˜ì„ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„

        Returns:
            AsyncIOMotorCollection: ìš”ì²­ëœ ì»¬ë ‰ì…˜ ê°ì²´
        """
        if self.main_database is None:
            raise DatabaseOperationException("Database not connected")
        return self.main_database[collection_name]

    async def create_collection_with_schema(
        self,
        collection_name: str,
        schema: Dict[str, Any] = None,
        indexes: List[Dict[str, Any]] = None,
    ) -> AsyncIOMotorCollection:
        """
        ìŠ¤í‚¤ë§ˆì™€ ì¸ë±ìŠ¤ë¥¼ í¬í•¨í•œ ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            schema: JSON ìŠ¤í‚¤ë§ˆ (ì„ íƒì‚¬í•­)
            indexes: ìƒì„±í•  ì¸ë±ìŠ¤ ëª©ë¡ (ì„ íƒì‚¬í•­)

        Returns:
            AsyncIOMotorCollection: ìƒì„±ëœ ì»¬ë ‰ì…˜
        """
        try:
            # ì»¬ë ‰ì…˜ ìƒì„± (ìŠ¤í‚¤ë§ˆ í¬í•¨)
            create_options = {}
            if schema:
                create_options["validator"] = {"$jsonSchema": schema}

            collection = await self.main_database.create_collection(
                collection_name, **create_options
            )

            # ì¸ë±ìŠ¤ ìƒì„±
            if indexes:
                for index in indexes:
                    await collection.create_index(
                        index.get("keys"), **index.get("options", {})
                    )

            logger.info(f"âœ… ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ: {collection_name}")
            return collection

        except Exception as e:
            if "already exists" in str(e):
                logger.info(f"ğŸ“‚ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚¬ìš©: {collection_name}")
                return self.get_collection(collection_name)
            else:
                logger.error(f"âŒ ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {collection_name}, ì˜¤ë¥˜: {e}")
                raise DatabaseOperationException(
                    f"ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {collection_name}", original_exception=e
                )

    async def auto_insert_document(
        self,
        collection_name: str,
        document: Dict[str, Any],
        ensure_collection: bool = True,
    ) -> str:
        """
        ì»¬ë ‰ì…˜ì— ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ì‚½ì… (ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±)

        Args:
            collection_name: ëŒ€ìƒ ì»¬ë ‰ì…˜ ì´ë¦„
            document: ì‚½ì…í•  ë¬¸ì„œ ë°ì´í„°
            ensure_collection: ì»¬ë ‰ì…˜ì´ ì—†ì„ ë•Œ ìë™ ìƒì„± ì—¬ë¶€

        Returns:
            str: ì‚½ì…ëœ ë¬¸ì„œì˜ ObjectId
        """
        try:
            # ìë™ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            if "created_at" not in document:
                document["created_at"] = datetime.now()
            if "updated_at" not in document:
                document["updated_at"] = datetime.now()

            # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            if ensure_collection:
                # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
                existing_collections = await self.main_database.list_collection_names()
                if collection_name not in existing_collections:
                    await self.create_collection_with_schema(collection_name)

            collection = self.get_collection(collection_name)

            # ë¬¸ì„œ ì‚½ì…
            result = await collection.insert_one(document)

            logger.info(
                f"ğŸ“ ë¬¸ì„œ ìë™ ì‚½ì… ì™„ë£Œ: {collection_name}, ID: {result.inserted_id}"
            )
            return str(result.inserted_id)

        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì‚½ì… ì‹¤íŒ¨: {collection_name}, ì˜¤ë¥˜: {e}")
            raise DatabaseOperationException(
                f"ë¬¸ì„œ ì‚½ì… ì‹¤íŒ¨: {collection_name}", original_exception=e
            )

    async def auto_insert_many_documents(
        self,
        collection_name: str,
        documents: List[Dict[str, Any]],
        ensure_collection: bool = True,
    ) -> List[str]:
        """
        ì»¬ë ‰ì…˜ì— ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ì‚½ì…

        Args:
            collection_name: ëŒ€ìƒ ì»¬ë ‰ì…˜ ì´ë¦„
            documents: ì‚½ì…í•  ë¬¸ì„œë“¤
            ensure_collection: ì»¬ë ‰ì…˜ì´ ì—†ì„ ë•Œ ìë™ ìƒì„± ì—¬ë¶€

        Returns:
            List[str]: ì‚½ì…ëœ ë¬¸ì„œë“¤ì˜ ObjectId ëª©ë¡
        """
        try:
            # ëª¨ë“  ë¬¸ì„œì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            now = datetime.now()
            for doc in documents:
                if "created_at" not in doc:
                    doc["created_at"] = now
                if "updated_at" not in doc:
                    doc["updated_at"] = now

            # ì»¬ë ‰ì…˜ í™•ì¸ ë° ìƒì„±
            if ensure_collection:
                existing_collections = await self.main_database.list_collection_names()
                if collection_name not in existing_collections:
                    await self.create_collection_with_schema(collection_name)

            collection = self.get_collection(collection_name)

            # ë¬¸ì„œë“¤ ì‚½ì…
            result = await collection.insert_many(documents)

            inserted_ids = [str(id) for id in result.inserted_ids]
            logger.info(
                f"ğŸ“ ë‹¤ì¤‘ ë¬¸ì„œ ìë™ ì‚½ì… ì™„ë£Œ: {collection_name}, ê°œìˆ˜: {len(inserted_ids)}"
            )
            return inserted_ids

        except Exception as e:
            logger.error(f"âŒ ë‹¤ì¤‘ ë¬¸ì„œ ì‚½ì… ì‹¤íŒ¨: {collection_name}, ì˜¤ë¥˜: {e}")
            raise DatabaseOperationException(
                f"ë‹¤ì¤‘ ë¬¸ì„œ ì‚½ì… ì‹¤íŒ¨: {collection_name}", original_exception=e
            )


class NotionSchemaCacheManager:
    """
    ë…¸ì…˜ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ ìºì‹±í•˜ì—¬ API í˜¸ì¶œ ìµœì†Œí™”

    ìºì‹± ì „ëµ:
    - TTL ê¸°ë°˜ ë§Œë£Œ (ê¸°ë³¸ 1ì‹œê°„)
    - ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì‹œ ì¦‰ì‹œ ë¬´íš¨í™”
    - ë©”ëª¨ë¦¬ + MongoDB ì´ì¤‘ ìºì‹± (í–¥í›„ í™•ì¥ ê°€ëŠ¥)

    ì„±ëŠ¥ íš¨ê³¼:
    - ë…¸ì…˜ API í˜¸ì¶œ 90% ì´ìƒ ê°ì†Œ
    - ì‘ë‹µ ì‹œê°„ 200ms â†’ 10ms ë‹¨ì¶•
    """

    def __init__(self, mongodb_connection: MongoDBConnectionManager):
        self.mongodb = mongodb_connection
        # í–¥í›„ ë©”ëª¨ë¦¬ ìºì‹œ ì¶”ê°€ ê°€ëŠ¥í•œ êµ¬ì¡°
        self.memory_cache = {}

    async def get_schema(self, notion_db_id: str) -> Optional[Dict[str, Any]]:
        """
        ìºì‹œëœ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì¡°íšŒí•˜ê³  ë§Œë£Œ í™•ì¸

        Args:
            notion_db_id: ë…¸ì…˜ ë°ì´í„°ë² ì´ìŠ¤ ê³ ìœ  ID

        Returns:
            Optional[Dict]: ìŠ¤í‚¤ë§ˆ ì •ë³´ ë˜ëŠ” None (ìºì‹œ ì—†ìŒ/ë§Œë£Œë¨)
        """
        try:
            # MongoDBì—ì„œ ìºì‹œ ë¬¸ì„œ ì¡°íšŒ
            cache_document = await self.mongodb.schema_cache_collection.find_one(
                {"db_id": notion_db_id}
            )

            if not cache_document:
                logger.debug(f"ğŸ” ìŠ¤í‚¤ë§ˆ ìºì‹œ ì—†ìŒ: {notion_db_id}")
                return None

            # TTL ê¸°ë°˜ ë§Œë£Œ í™•ì¸
            expiry_time = cache_document["created_at"] + timedelta(
                seconds=settings.schema_cache_ttl
            )
            current_time = datetime.utcnow()

            if current_time > expiry_time:
                # ë§Œë£Œëœ ìºì‹œ ìë™ ì‚­ì œ
                await self.mongodb.schema_cache_collection.delete_one(
                    {"db_id": notion_db_id}
                )
                logger.debug(f"â° ìŠ¤í‚¤ë§ˆ ìºì‹œ ë§Œë£Œë¡œ ì‚­ì œ: {notion_db_id}")
                return None

            logger.debug(f"âœ… ìŠ¤í‚¤ë§ˆ ìºì‹œ íˆíŠ¸: {notion_db_id}")
            return cache_document["schema"]

        except Exception as lookup_error:
            logger.error(f"âŒ ìŠ¤í‚¤ë§ˆ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨ {notion_db_id}: {lookup_error}")
            return None

    async def save_schema(self, notion_db_id: str, schema_data: Dict[str, Any]):
        """
        ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ìºì‹œì— ì €ì¥ (upsert ë°©ì‹)

        Args:
            notion_db_id: ë…¸ì…˜ ë°ì´í„°ë² ì´ìŠ¤ ê³ ìœ  ID
            schema_data: ìºì‹±í•  ìŠ¤í‚¤ë§ˆ ì •ë³´
        """
        try:
            current_time = datetime.utcnow()
            cache_document = {
                "db_id": notion_db_id,
                "schema": schema_data,
                "created_at": current_time,
                "updated_at": current_time,
                # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                "cache_hit_count": 0,
                "last_accessed": current_time,
            }

            # upsert: ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…
            await self.mongodb.schema_cache_collection.replace_one(
                {"db_id": notion_db_id}, cache_document, upsert=True
            )

            logger.debug(f"ğŸ’¾ ìŠ¤í‚¤ë§ˆ ìºì‹œ ì €ì¥ ì™„ë£Œ: {notion_db_id}")

        except Exception as save_error:
            logger.error(f"âŒ ìŠ¤í‚¤ë§ˆ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ {notion_db_id}: {save_error}")
            # ìºì‹œ ì €ì¥ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚¤ì§€ ì•ŠìŒ

    async def invalidate_schema_cache(self, notion_db_id: str):
        """
        íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ì˜ ìºì‹œë¥¼ ê°•ì œë¡œ ë¬´íš¨í™”

        ì‚¬ìš© ì‹œì :
        - ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸ í›„ (select ì˜µì…˜ ì¶”ê°€ ë“±)
        - ì—ëŸ¬ ë°œìƒ ì‹œ ìºì‹œ ì´ˆê¸°í™”

        Args:
            notion_db_id: ë¬´íš¨í™”í•  ë°ì´í„°ë² ì´ìŠ¤ ID
        """
        try:
            delete_result = await self.mongodb.schema_cache_collection.delete_one(
                {"db_id": notion_db_id}
            )

            if delete_result.deleted_count > 0:
                logger.info(f"ğŸ—‘ï¸  ìŠ¤í‚¤ë§ˆ ìºì‹œ ë¬´íš¨í™” ì™„ë£Œ: {notion_db_id}")
            else:
                logger.debug(f"ğŸ¤· ë¬´íš¨í™”í•  ìŠ¤í‚¤ë§ˆ ìºì‹œ ì—†ìŒ: {notion_db_id}")

        except Exception as invalidate_error:
            logger.error(
                f"âŒ ìŠ¤í‚¤ë§ˆ ìºì‹œ ë¬´íš¨í™” ì‹¤íŒ¨ {notion_db_id}: {invalidate_error}"
            )

    async def get_cache_stats(self) -> Dict[str, Any]:
        """
        ìºì‹œ ì‚¬ìš© í†µê³„ ì¡°íšŒ (ëª¨ë‹ˆí„°ë§ìš©)

        Returns:
            Dict: ìºì‹œ íˆíŠ¸ìœ¨, ì €ì¥ëœ ìŠ¤í‚¤ë§ˆ ìˆ˜ ë“±
        """
        try:
            total_cache_count = (
                await self.mongodb.schema_cache_collection.count_documents({})
            )

            # ìµœê·¼ ì ‘ê·¼ëœ ìºì‹œë“¤
            recent_accessed_cache = (
                await self.mongodb.schema_cache_collection.find(
                    {}, {"db_id": 1, "last_accessed": 1, "cache_hit_count": 1}
                )
                .sort("last_accessed", -1)
                .limit(5)
                .to_list(5)
            )

            return {
                "total_cache_count": total_cache_count,
                "recent_accessed_cache": recent_accessed_cache,
                "cache_ttl_setting": f"{settings.schema_cache_ttl}ì´ˆ",
            }
        except Exception as stats_error:
            logger.error(f"âŒ ìºì‹œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {stats_error}")
            return {"error": str(stats_error)}


class DiscordThreadCacheManager:
    """
    ë””ìŠ¤ì½”ë“œ ìŠ¤ë ˆë“œ ì •ë³´ë¥¼ ìºì‹±í•˜ì—¬ ì¤‘ë³µ ìƒì„± ë°©ì§€ ë° ì„±ëŠ¥ í–¥ìƒ

    ì£¼ìš” ê¸°ëŠ¥:
    - ì¼ì¼ ìŠ¤ë ˆë“œ ìë™ ìƒì„±/ì¡°íšŒ ìµœì í™”
    - ìŠ¤ë ˆë“œ ì‚¬ìš© ë¹ˆë„ ì¶”ì 
    - ì±„ë„ë³„ ìŠ¤ë ˆë“œ ê´€ë¦¬

    ìºì‹œ í‚¤: (channel_id, thread_name) ì¡°í•©
    """

    def __init__(self, mongodb_connection: MongoDBConnectionManager):
        self.mongodb = mongodb_connection

    async def get_thread_info(
        self, channel_id: int, thread_name: str
    ) -> Optional[Dict[str, Any]]:
        """
        ìºì‹œì—ì„œ ìŠ¤ë ˆë“œ ì •ë³´ ì¡°íšŒ

        Args:
            channel_id: ë””ìŠ¤ì½”ë“œ ì±„ë„ ID
            thread_name: ìŠ¤ë ˆë“œ ì´ë¦„ (ì˜ˆ: "2024/01/15")

        Returns:
            Optional[Dict]: ìŠ¤ë ˆë“œ ì •ë³´ (thread_id, ìƒì„±ì‹œê°„ ë“±) ë˜ëŠ” None
        """
        try:
            thread_document = await self.mongodb.thread_cache_collection.find_one(
                {"channel_id": channel_id, "thread_name": thread_name}
            )

            if thread_document:
                logger.debug(f"ğŸ¯ ìŠ¤ë ˆë“œ ìºì‹œ íˆíŠ¸: {thread_name} in {channel_id}")
            else:
                logger.debug(f"ğŸ” ìŠ¤ë ˆë“œ ìºì‹œ ë¯¸ìŠ¤: {thread_name} in {channel_id}")

            return thread_document

        except Exception as lookup_error:
            logger.error(f"âŒ ìŠ¤ë ˆë“œ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {lookup_error}")
            return None

    async def save_thread_info(self, channel_id: int, thread_name: str, thread_id: int):
        """
        ìƒˆë¡œ ìƒì„±ëœ ìŠ¤ë ˆë“œ ì •ë³´ë¥¼ ìºì‹œì— ì €ì¥

        Args:
            channel_id: ë””ìŠ¤ì½”ë“œ ì±„ë„ ID
            thread_name: ìŠ¤ë ˆë“œ ì´ë¦„
            thread_id: ìƒì„±ëœ ìŠ¤ë ˆë“œ ID
        """
        try:
            current_time = datetime.utcnow()
            thread_document = {
                "channel_id": channel_id,
                "thread_name": thread_name,
                "thread_id": thread_id,
                "created_at": current_time,
                "last_used": current_time,
                "use_count": 1,  # ì‚¬ìš© íšŸìˆ˜ ì¶”ì 
            }

            # upsertë¡œ ê¸°ì¡´ ì •ë³´ ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒˆë¡œ ì‚½ì…
            await self.mongodb.thread_cache_collection.replace_one(
                {"channel_id": channel_id, "thread_name": thread_name},
                thread_document,
                upsert=True,
            )

            logger.debug(f"ğŸ’¾ ìŠ¤ë ˆë“œ ìºì‹œ ì €ì¥: {thread_name} (ID: {thread_id})")

        except Exception as save_error:
            logger.error(f"âŒ ìŠ¤ë ˆë“œ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {save_error}")

    async def update_thread_usage_time(self, channel_id: int, thread_name: str):
        """
        ìŠ¤ë ˆë“œ ìµœê·¼ ì‚¬ìš© ì‹œê°„ê³¼ ì‚¬ìš© íšŸìˆ˜ ì—…ë°ì´íŠ¸

        Args:
            channel_id: ë””ìŠ¤ì½”ë“œ ì±„ë„ ID
            thread_name: ìŠ¤ë ˆë“œ ì´ë¦„
        """
        try:
            update_result = await self.mongodb.thread_cache_collection.update_one(
                {"channel_id": channel_id, "thread_name": thread_name},
                {
                    "$set": {"last_used": datetime.utcnow()},
                    "$inc": {"use_count": 1},  # ì‚¬ìš© íšŸìˆ˜ 1 ì¦ê°€
                },
            )

            if update_result.modified_count > 0:
                logger.debug(f"ğŸ”„ ìŠ¤ë ˆë“œ ì‚¬ìš© ì‹œê°„ ì—…ë°ì´íŠ¸: {thread_name}")

        except Exception as update_error:
            logger.error(f"âŒ ìŠ¤ë ˆë“œ ì‚¬ìš© ì‹œê°„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_error}")

    async def cleanup_old_thread_cache(self, retention_days: int = 30):
        """
        ì§€ì •ëœ ê¸°ê°„ë³´ë‹¤ ì˜¤ë˜ëœ ìŠ¤ë ˆë“œ ìºì‹œ ìë™ ì •ë¦¬

        Args:
            retention_days: ìºì‹œ ë³´ê´€ ê¸°ê°„ (ê¸°ë³¸ 30ì¼)
        """
        try:
            cutoff_time = datetime.utcnow() - timedelta(days=retention_days)

            delete_result = await self.mongodb.thread_cache_collection.delete_many(
                {"last_used": {"$lt": cutoff_time}}
            )

            if delete_result.deleted_count > 0:
                logger.info(
                    f"ğŸ§¹ ì˜¤ë˜ëœ ìŠ¤ë ˆë“œ ìºì‹œ ì •ë¦¬: {delete_result.deleted_count}ê°œ ì‚­ì œ"
                )

        except Exception as cleanup_error:
            logger.error(f"âŒ ìŠ¤ë ˆë“œ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {cleanup_error}")


class PerformanceMetricsCollector:
    """
    ì• í”Œë¦¬ì¼€ì´ì…˜ ì„±ëŠ¥ ë° ì‚¬ìš© í†µê³„ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì œê³µ

    ìˆ˜ì§‘ ë°ì´í„°:
    - ë””ìŠ¤ì½”ë“œ ëª…ë ¹ì–´ ì‚¬ìš© í†µê³„
    - ì›¹í›… í˜¸ì¶œ í†µê³„
    - ì—ëŸ¬ ë°œìƒ ë¹ˆë„
    - ì‘ë‹µ ì‹œê°„ ë©”íŠ¸ë¦­
    """

    def __init__(self, mongodb_connection: MongoDBConnectionManager):
        self.mongodb = mongodb_connection

    async def record_command_usage(
        self,
        command_name: str,
        user_id: int,
        guild_id: int,
        success: bool = True,
        execution_time_seconds: Optional[float] = None,
    ):
        """
        ë””ìŠ¤ì½”ë“œ ìŠ¬ë˜ì‹œ ëª…ë ¹ì–´ ì‚¬ìš© í†µê³„ ê¸°ë¡

        Args:
            command_name: ì‹¤í–‰ëœ ëª…ë ¹ì–´ ì´ë¦„ (ì˜ˆ: "task", "meeting")
            user_id: ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•œ ì‚¬ìš©ì Discord ID
            guild_id: ëª…ë ¹ì–´ê°€ ì‹¤í–‰ëœ ì„œë²„ Discord ID
            success: ëª…ë ¹ì–´ ì‹¤í–‰ ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€
            execution_time_seconds: ëª…ë ¹ì–´ ì‹¤í–‰ì— ê±¸ë¦° ì‹œê°„ (ì„±ëŠ¥ ë¶„ì„ìš©)
        """
        try:
            metric_document = {
                "type": "command_usage",
                "command": command_name,
                "user_id": user_id,
                "guild_id": guild_id,
                "success": success,
                "execution_time": execution_time_seconds,
                "timestamp": datetime.utcnow(),
            }

            await self.mongodb.metrics_collection.insert_one(metric_document)
            logger.debug(
                f"ğŸ“Š ëª…ë ¹ì–´ ì‚¬ìš© ê¸°ë¡: {command_name} ({'ì„±ê³µ' if success else 'ì‹¤íŒ¨'})"
            )

        except Exception as record_error:
            logger.error(f"âŒ ëª…ë ¹ì–´ ì‚¬ìš© ê¸°ë¡ ì‹¤íŒ¨: {record_error}")

    async def record_webhook_call(
        self,
        page_id: str,
        channel_id: int,
        success: bool = True,
        processing_time_seconds: Optional[float] = None,
    ):
        """
        ë…¸ì…˜ ì›¹í›… í˜¸ì¶œ í†µê³„ ê¸°ë¡

        Args:
            page_id: ë…¸ì…˜ í˜ì´ì§€ ID
            channel_id: ë©”ì‹œì§€ê°€ ì „ì†¡ëœ ë””ìŠ¤ì½”ë“œ ì±„ë„ ID
            success: ì›¹í›… ì²˜ë¦¬ ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€
            processing_time_seconds: ì›¹í›… ì²˜ë¦¬ì— ê±¸ë¦° ì‹œê°„
        """
        try:
            metric_document = {
                "type": "webhook_call",
                "page_id": page_id,
                "channel_id": channel_id,
                "success": success,
                "processing_time": processing_time_seconds,
                "timestamp": datetime.utcnow(),
            }

            await self.mongodb.metrics_collection.insert_one(metric_document)
            logger.debug(
                f"ğŸ“ˆ ì›¹í›… í˜¸ì¶œ ê¸°ë¡: {page_id} ({'ì„±ê³µ' if success else 'ì‹¤íŒ¨'})"
            )

        except Exception as record_error:
            logger.error(f"âŒ ì›¹í›… í˜¸ì¶œ ê¸°ë¡ ì‹¤íŒ¨: {record_error}")

    async def record_error(
        self, error_category: str, error_message: str, details: Dict[str, Any] = None
    ):
        """
        ì—ëŸ¬ ë°œìƒ í†µê³„ ê¸°ë¡ (ê¸€ë¡œë²Œ ì˜ˆì™¸ í•¸ë“¤ëŸ¬ì™€ ì—°ë™)

        Args:
            error_category: ì—ëŸ¬ ë¶„ë¥˜ (ì˜ˆ: "notion_api_error")
            error_message: ì—ëŸ¬ ë©”ì‹œì§€
            details: ì¶”ê°€ ë””ë²„ê¹… ì •ë³´
        """
        try:
            metric_document = {
                "type": "error_occurrence",
                "error_category": error_category,
                "error_message": error_message,
                "details": details or {},
                "timestamp": datetime.utcnow(),
            }

            await self.mongodb.metrics_collection.insert_one(metric_document)

        except Exception as record_error:
            logger.error(f"âŒ ì—ëŸ¬ ë°œìƒ ê¸°ë¡ ì‹¤íŒ¨: {record_error}")

    async def get_daily_stats(self, date: Optional[datetime] = None) -> Dict[str, Any]:
        """
        íŠ¹ì • ë‚ ì§œì˜ ì‚¬ìš© í†µê³„ ì¡°íšŒ

        Args:
            date: ì¡°íšŒí•  ë‚ ì§œ (ê¸°ë³¸ê°’: ì˜¤ëŠ˜)

        Returns:
            Dict: ì¼ì¼ í†µê³„ ë°ì´í„°
        """
        if not date:
            date = datetime.utcnow()

        start_time = date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_time = start_time + timedelta(days=1)

        try:
            # ì§‘ê³„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í†µê³„ ê³„ì‚°
            pipeline = [
                {"$match": {"timestamp": {"$gte": start_time, "$lt": end_time}}},
                {
                    "$group": {
                        "_id": "$type",
                        "count": {"$sum": 1},
                        "success_count": {
                            "$sum": {"$cond": [{"$eq": ["$success", True]}, 1, 0]}
                        },
                    }
                },
            ]

            aggregation_results = await self.mongodb.metrics_collection.aggregate(
                pipeline
            ).to_list(100)

            # ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬
            stats_data = {}
            for result in aggregation_results:
                type_name = result["_id"]
                stats_data[type_name] = {
                    "total_calls": result["count"],
                    "successful_calls": result["success_count"],
                    "success_rate": (
                        round(result["success_count"] / result["count"] * 100, 2)
                        if result["count"] > 0
                        else 0
                    ),
                }

            return {"date": date.strftime("%Y-%m-%d"), "stats": stats_data}

        except Exception as lookup_error:
            logger.error(f"âŒ ì¼ì¼ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {lookup_error}")
            return {"error": str(lookup_error)}


# Global instances (used throughout the application)
mongodb_connection = MongoDBConnectionManager()
schema_cache_manager = NotionSchemaCacheManager(mongodb_connection)
thread_cache_manager = DiscordThreadCacheManager(mongodb_connection)
metrics_collector = PerformanceMetricsCollector(mongodb_connection)


# ===== DinoBot ì„œë¹„ìŠ¤ ì „ìš© ì»¬ë ‰ì…˜ ì •ì˜ =====

MEETUP_LOADER_COLLECTIONS = {
    "discord_commands": {
        "description": "Discord ëª…ë ¹ì–´ ì‹¤í–‰ ë¡œê·¸",
        "schema": {
            "bsonType": "object",
            "required": ["command_type", "user_id", "guild_id", "timestamp"],
            "properties": {
                "command_type": {
                    "bsonType": "string",
                    "description": "ëª…ë ¹ì–´ íƒ€ì… (task/meeting/status)",
                },
                "user_id": {"bsonType": "long", "description": "Discord ì‚¬ìš©ì ID"},
                "guild_id": {"bsonType": "long", "description": "Discord ì„œë²„ ID"},
                "channel_id": {"bsonType": "long", "description": "ì‹¤í–‰ëœ ì±„ë„ ID"},
                "parameters": {"bsonType": "object", "description": "ëª…ë ¹ì–´ ë§¤ê°œë³€ìˆ˜"},
                "success": {"bsonType": "bool", "description": "ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€"},
                "execution_time_ms": {
                    "bsonType": "double",
                    "description": "ì‹¤í–‰ ì‹œê°„ (ë°€ë¦¬ì´ˆ)",
                },
                "error_message": {
                    "bsonType": "string",
                    "description": "ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)",
                },
                "timestamp": {"bsonType": "date", "description": "ì‹¤í–‰ ì‹œê°„"},
                "created_at": {"bsonType": "date", "description": "ìƒì„± ì‹œê°„"},
                "updated_at": {"bsonType": "date", "description": "ìˆ˜ì • ì‹œê°„"},
            },
        },
        "indexes": [
            {"keys": [("command_type", 1)], "options": {}},
            {"keys": [("user_id", 1)], "options": {}},
            {"keys": [("guild_id", 1)], "options": {}},
            {"keys": [("timestamp", -1)], "options": {}},
            {"keys": [("success", 1)], "options": {}},
        ],
    },
    "notion_pages": {
        "description": "ìƒì„±ëœ Notion í˜ì´ì§€ ë¡œê·¸",
        "schema": {
            "bsonType": "object",
            "required": ["page_id", "database_id", "page_type", "created_by"],
            "properties": {
                "page_id": {"bsonType": "string", "description": "Notion í˜ì´ì§€ ID"},
                "database_id": {
                    "bsonType": "string",
                    "description": "Notion ë°ì´í„°ë² ì´ìŠ¤ ID",
                },
                "page_type": {
                    "bsonType": "string",
                    "description": "í˜ì´ì§€ íƒ€ì… (task/meeting)",
                },
                "title": {"bsonType": "string", "description": "í˜ì´ì§€ ì œëª©"},
                "page_url": {"bsonType": "string", "description": "í˜ì´ì§€ URL"},
                "created_by": {
                    "bsonType": "string",
                    "description": "ìƒì„±í•œ Discord ì‚¬ìš©ì ID",
                },
                "properties": {"bsonType": "object", "description": "ì„¤ì •ëœ ì†ì„±ë“¤"},
                "status": {"bsonType": "string", "description": "í˜ì´ì§€ ìƒíƒœ"},
                "created_at": {"bsonType": "date", "description": "ìƒì„± ì‹œê°„"},
                "updated_at": {"bsonType": "date", "description": "ìˆ˜ì • ì‹œê°„"},
            },
        },
        "indexes": [
            {"keys": [("page_id", 1)], "options": {"unique": True}},
            {"keys": [("database_id", 1)], "options": {}},
            {"keys": [("page_type", 1)], "options": {}},
            {"keys": [("created_by", 1)], "options": {}},
            {"keys": [("created_at", -1)], "options": {}},
        ],
    },
    "webhook_calls": {
        "description": "Notion ì›¹í›… í˜¸ì¶œ ë¡œê·¸",
        "schema": {
            "bsonType": "object",
            "required": ["page_id", "channel_id", "request_time"],
            "properties": {
                "page_id": {"bsonType": "string", "description": "Notion í˜ì´ì§€ ID"},
                "channel_id": {"bsonType": "long", "description": "Discord ì±„ë„ ID"},
                "thread_id": {"bsonType": "long", "description": "Discord ìŠ¤ë ˆë“œ ID"},
                "mode": {"bsonType": "string", "description": "ì²˜ë¦¬ ëª¨ë“œ"},
                "success": {"bsonType": "bool", "description": "ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€"},
                "extracted_text_length": {
                    "bsonType": "int",
                    "description": "ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´",
                },
                "processing_time_ms": {
                    "bsonType": "double",
                    "description": "ì²˜ë¦¬ ì‹œê°„ (ë°€ë¦¬ì´ˆ)",
                },
                "error_code": {
                    "bsonType": "string",
                    "description": "ì—ëŸ¬ ì½”ë“œ (ì‹¤íŒ¨ ì‹œ)",
                },
                "request_ip": {"bsonType": "string", "description": "ìš”ì²­ IP"},
                "request_time": {"bsonType": "date", "description": "ìš”ì²­ ì‹œê°„"},
                "created_at": {"bsonType": "date", "description": "ìƒì„± ì‹œê°„"},
                "updated_at": {"bsonType": "date", "description": "ìˆ˜ì • ì‹œê°„"},
            },
        },
        "indexes": [
            {"keys": [("page_id", 1)], "options": {}},
            {"keys": [("channel_id", 1)], "options": {}},
            {"keys": [("success", 1)], "options": {}},
            {"keys": [("request_time", -1)], "options": {}},
            {"keys": [("processing_time_ms", 1)], "options": {}},
        ],
    },
    "user_preferences": {
        "description": "Discord ì‚¬ìš©ì ì„¤ì • ë° í”„ë¡œí•„",
        "schema": {
            "bsonType": "object",
            "required": ["user_id", "username"],
            "properties": {
                "user_id": {"bsonType": "long", "description": "Discord ì‚¬ìš©ì ID"},
                "username": {"bsonType": "string", "description": "Discord ì‚¬ìš©ìëª…"},
                "display_name": {"bsonType": "string", "description": "í‘œì‹œëª…"},
                "avatar_url": {"bsonType": "string", "description": "ì•„ë°”íƒ€ URL"},
                "preferences": {
                    "bsonType": "object",
                    "description": "ì‚¬ìš©ì ì„¤ì •",
                    "properties": {
                        "language": {"bsonType": "string", "description": "ì–¸ì–´ ì„¤ì •"},
                        "timezone": {
                            "bsonType": "string",
                            "description": "ì‹œê°„ëŒ€ ì„¤ì •",
                        },
                        "notification_enabled": {
                            "bsonType": "bool",
                            "description": "ì•Œë¦¼ í™œì„±í™”",
                        },
                        "default_database": {
                            "bsonType": "string",
                            "description": "ê¸°ë³¸ Notion ë°ì´í„°ë² ì´ìŠ¤",
                        },
                    },
                },
                "last_active": {"bsonType": "date", "description": "ë§ˆì§€ë§‰ í™œë™ ì‹œê°„"},
                "command_count": {
                    "bsonType": "int",
                    "description": "ì´ ëª…ë ¹ì–´ ì‚¬ìš© íšŸìˆ˜",
                },
                "created_at": {"bsonType": "date", "description": "ìƒì„± ì‹œê°„"},
                "updated_at": {"bsonType": "date", "description": "ìˆ˜ì • ì‹œê°„"},
            },
        },
        "indexes": [
            {"keys": [("user_id", 1)], "options": {"unique": True}},
            {"keys": [("username", 1)], "options": {}},
            {"keys": [("last_active", -1)], "options": {}},
            {"keys": [("command_count", -1)], "options": {}},
        ],
    },
    "system_events": {
        "description": "ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ ë¡œê·¸",
        "schema": {
            "bsonType": "object",
            "required": ["event_type", "timestamp"],
            "properties": {
                "event_type": {"bsonType": "string", "description": "ì´ë²¤íŠ¸ íƒ€ì…"},
                "event_category": {
                    "bsonType": "string",
                    "description": "ì´ë²¤íŠ¸ ì¹´í…Œê³ ë¦¬",
                },
                "description": {"bsonType": "string", "description": "ì´ë²¤íŠ¸ ì„¤ëª…"},
                "severity": {
                    "bsonType": "string",
                    "description": "ì‹¬ê°ë„ (info/warning/error/critical)",
                },
                "source": {"bsonType": "string", "description": "ì´ë²¤íŠ¸ ì†ŒìŠ¤"},
                "metadata": {"bsonType": "object", "description": "ì¶”ê°€ ë©”íƒ€ë°ì´í„°"},
                "user_id": {
                    "bsonType": ["long", "null"],
                    "description": "ê´€ë ¨ ì‚¬ìš©ì ID (ì„ íƒì‚¬í•­)",
                },
                "guild_id": {
                    "bsonType": ["long", "null"],
                    "description": "ê´€ë ¨ ì„œë²„ ID (ì„ íƒì‚¬í•­)",
                },
                "timestamp": {"bsonType": "date", "description": "ì´ë²¤íŠ¸ ë°œìƒ ì‹œê°„"},
                "created_at": {"bsonType": "date", "description": "ìƒì„± ì‹œê°„"},
            },
        },
        "indexes": [
            {"keys": [("event_type", 1)], "options": {}},
            {"keys": [("event_category", 1)], "options": {}},
            {"keys": [("severity", 1)], "options": {}},
            {"keys": [("timestamp", -1)], "options": {}},
            {"keys": [("source", 1)], "options": {}},
        ],
    },
    "page_content_cache": {
        "description": "Notion í˜ì´ì§€ ë‚´ìš© ìºì‹œ",
        "schema": {
            "bsonType": "object",
            "required": ["page_id", "content", "cached_at"],
            "properties": {
                "page_id": {"bsonType": "string", "description": "Notion í˜ì´ì§€ ID"},
                "content": {"bsonType": "string", "description": "ìºì‹œëœ í˜ì´ì§€ ë‚´ìš©"},
                "content_length": {"bsonType": "int", "description": "ë‚´ìš© ê¸¸ì´"},
                "cached_at": {
                    "bsonType": "double",
                    "description": "ìºì‹œëœ ì‹œê°„ (timestamp)",
                },
                "expires_at": {
                    "bsonType": "double",
                    "description": "ë§Œë£Œ ì‹œê°„ (timestamp)",
                },
                "created_at": {"bsonType": "date", "description": "ìƒì„± ì‹œê°„"},
            },
        },
        "indexes": [
            {"keys": [("page_id", 1)], "options": {"unique": True}},
            {"keys": [("cached_at", -1)], "options": {}},
            {
                "keys": [("expires_at", 1)],
                "options": {"expireAfterSeconds": 0},
            },  # TTL index
        ],
    },
}


async def initialize_meetup_loader_collections():
    """DinoBot ì„œë¹„ìŠ¤ì— í•„ìš”í•œ ëª¨ë“  ì»¬ë ‰ì…˜ì„ ì´ˆê¸°í™”"""
    if mongodb_connection.main_database is None:
        logger.error("âŒ MongoDB ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤")
        raise DatabaseOperationException("MongoDB not connected")

    created_collections = []
    existing_collections = []
    errors = []

    try:
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ
        existing_collection_names = (
            await mongodb_connection.main_database.list_collection_names()
        )

        for collection_name, config in MEETUP_LOADER_COLLECTIONS.items():
            try:
                if collection_name in existing_collection_names:
                    existing_collections.append(collection_name)
                else:
                    # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
                    await mongodb_connection.create_collection_with_schema(
                        collection_name, config["schema"], config["indexes"]
                    )
                    created_collections.append(collection_name)

            except Exception as e:
                error_msg = f"ì»¬ë ‰ì…˜ {collection_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"
                logger.error(f"âŒ {error_msg}")
                errors.append(error_msg)

        # ê²°ê³¼ ìš”ì•½
        total_collections = len(MEETUP_LOADER_COLLECTIONS)
        logger.info(f"ğŸ‰ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info(f"   ğŸ“Š ì´ ì»¬ë ‰ì…˜: {total_collections}ê°œ")
        logger.info(f"   âœ… ìƒˆë¡œ ìƒì„±: {len(created_collections)}ê°œ")
        logger.info(f"   ğŸ“‚ ê¸°ì¡´ ìœ ì§€: {len(existing_collections)}ê°œ")

        if errors:
            logger.warning(f"   âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {len(errors)}ê°œ")
            for error in errors:
                logger.warning(f"      - {error}")

        return {
            "total_collections": total_collections,
            "created_collections": created_collections,
            "existing_collections": existing_collections,
            "errors": errors,
            "success": len(errors) == 0,
        }

    except Exception as e:
        logger.error(f"âŒ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
        raise DatabaseOperationException(
            "Collection initialization failed", original_exception=e
        )


def get_meetup_collection(collection_name: str) -> AsyncIOMotorCollection:
    """DinoBot ì„œë¹„ìŠ¤ ì»¬ë ‰ì…˜ì— ì•ˆì „í•˜ê²Œ ì ‘ê·¼"""
    if collection_name not in MEETUP_LOADER_COLLECTIONS:
        available = list(MEETUP_LOADER_COLLECTIONS.keys())
        raise ValueError(
            f"ì•Œ ìˆ˜ ì—†ëŠ” ì»¬ë ‰ì…˜: {collection_name}. ì‚¬ìš© ê°€ëŠ¥: {available}"
        )

    return mongodb_connection.get_collection(collection_name)


# ===== DinoBot ì„œë¹„ìŠ¤ ì „ìš© ë°ì´í„° ì €ì¥ í•¨ìˆ˜ë“¤ =====


async def log_discord_command(
    command_type: str,
    user_id: int,
    guild_id: int,
    channel_id: int,
    parameters: Dict[str, Any],
    success: bool,
    execution_time_ms: float,
    error_message: str = None,
) -> str:
    """Discord ëª…ë ¹ì–´ ì‹¤í–‰ ë¡œê·¸ ì €ì¥"""
    document = {
        "command_type": command_type,
        "user_id": user_id,
        "guild_id": guild_id,
        "channel_id": channel_id,
        "parameters": parameters,
        "success": success,
        "execution_time_ms": execution_time_ms,
        "error_message": error_message,
        "timestamp": datetime.now(),
    }

    collection = get_meetup_collection("discord_commands")
    result = await collection.insert_one(document)
    return str(result.inserted_id)


async def log_notion_page(
    page_id: str,
    database_id: str,
    page_type: str,
    title: str,
    page_url: str,
    created_by: int,
    properties: Dict[str, Any] = None,
    status: str = "created",
) -> str:
    """ìƒì„±ëœ Notion í˜ì´ì§€ ë¡œê·¸ ì €ì¥"""
    document = {
        "page_id": page_id,
        "database_id": database_id,
        "page_type": page_type,
        "title": title,
        "page_url": page_url,
        "created_by": created_by,
        "properties": properties or {},
        "status": status,
    }

    collection = get_meetup_collection("notion_pages")
    result = await collection.insert_one(document)
    return str(result.inserted_id)


async def log_webhook_call(
    page_id: str,
    channel_id: int,
    thread_id: int = None,
    mode: str = "meeting",
    success: bool = True,
    extracted_text_length: int = None,
    processing_time_ms: float = 0,
    error_code: str = None,
    request_ip: str = None,
) -> str:
    """Notion ì›¹í›… í˜¸ì¶œ ë¡œê·¸ ì €ì¥"""
    document = {
        "page_id": page_id,
        "channel_id": channel_id,
        "thread_id": thread_id,
        "mode": mode,
        "success": success,
        "extracted_text_length": extracted_text_length,
        "processing_time_ms": processing_time_ms,
        "error_code": error_code,
        "request_ip": request_ip,
        "request_time": datetime.now(),
    }

    collection = get_meetup_collection("webhook_calls")
    result = await collection.insert_one(document)
    return str(result.inserted_id)


async def save_user_preferences(
    user_id: int,
    username: str,
    display_name: str = None,
    avatar_url: str = None,
    preferences: Dict[str, Any] = None,
) -> str:
    """ì‚¬ìš©ì ì„¤ì • ì €ì¥ (upsert)"""
    document = {
        "user_id": user_id,
        "username": username,
        "display_name": display_name,
        "avatar_url": avatar_url,
        "preferences": preferences or {},
        "last_active": datetime.now(),
        "$inc": {"command_count": 1},  # ëª…ë ¹ì–´ ì‚¬ìš© íšŸìˆ˜ ì¦ê°€
    }

    collection = get_meetup_collection("user_preferences")

    # upsert ì‚¬ìš© (ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ìƒì„±)
    result = await collection.update_one(
        {"user_id": user_id},
        {"$set": document, "$setOnInsert": {"created_at": datetime.now()}},
        upsert=True,
    )

    return str(result.upserted_id if result.upserted_id else user_id)


async def log_system_event(
    event_type: str,
    description: str,
    severity: str = "info",
    event_category: str = "system",
    source: str = "dinobot",
    metadata: Dict[str, Any] = None,
    user_id: int = None,
    guild_id: int = None,
) -> str:
    """ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ ë¡œê·¸ ì €ì¥"""
    document = {
        "event_type": event_type,
        "event_category": event_category,
        "description": description,
        "severity": severity,
        "source": source,
        "metadata": metadata or {},
        "timestamp": datetime.now(),
    }

    # Only add user_id and guild_id if they are not None
    if user_id is not None:
        document["user_id"] = user_id
    if guild_id is not None:
        document["guild_id"] = guild_id

    collection = get_meetup_collection("system_events")
    result = await collection.insert_one(document)
    return str(result.inserted_id)


# ===== ì¡°íšŒ ë° í†µê³„ í•¨ìˆ˜ë“¤ =====


async def get_user_command_history(
    user_id: int, limit: int = 50
) -> List[Dict[str, Any]]:
    """ì‚¬ìš©ìì˜ ëª…ë ¹ì–´ ì‹¤í–‰ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    collection = get_meetup_collection("discord_commands")
    cursor = collection.find({"user_id": user_id}).sort("timestamp", -1).limit(limit)

    documents = await cursor.to_list(length=limit)
    for doc in documents:
        doc["_id"] = str(doc["_id"])

    return documents


async def save_notion_page(
    page_id: str,
    database_id: str,
    page_type: str,
    title: str,
    created_by: str,
    metadata: Optional[Dict[str, Any]] = None,
) -> str:
    """ë…¸ì…˜ í˜ì´ì§€ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    try:
        collection = get_meetup_collection("notion_pages")

        page_document = {
            "page_id": page_id,
            "database_id": database_id,
            "page_type": page_type,  # "task", "meeting", etc.
            "title": title,
            "created_by": created_by,
            "created_at": datetime.now(),
            "metadata": metadata or {},
        }

        result = await collection.insert_one(page_document)
        logger.info(f"ğŸ“ ë…¸ì…˜ í˜ì´ì§€ ì €ì¥ ì™„ë£Œ: {title} (ID: {page_id})")
        return str(result.inserted_id)

    except Exception as e:
        logger.error(f"âŒ ë…¸ì…˜ í˜ì´ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
        raise DatabaseOperationException(f"ë…¸ì…˜ í˜ì´ì§€ ì €ì¥ ì‹¤íŒ¨", original_exception=e)


async def get_recent_notion_pages(limit: int = 20) -> List[Dict[str, Any]]:
    """ìµœê·¼ ìƒì„±ëœ Notion í˜ì´ì§€ ëª©ë¡ ì¡°íšŒ"""
    collection = get_meetup_collection("notion_pages")
    cursor = collection.find().sort("created_at", -1).limit(limit)

    documents = await cursor.to_list(length=limit)
    for doc in documents:
        doc["_id"] = str(doc["_id"])

    return documents


async def get_recent_notion_page_by_user(
    user_id: str, limit: int = 5
) -> Optional[Dict[str, Any]]:
    """íŠ¹ì • ì‚¬ìš©ìê°€ ìµœê·¼ì— ìƒì„±í•œ ë…¸ì…˜ í˜ì´ì§€ ì¡°íšŒ"""
    try:
        collection = get_meetup_collection("notion_pages")
        cursor = (
            collection.find({"created_by": user_id}).sort("created_at", -1).limit(limit)
        )

        documents = await cursor.to_list(length=limit)
        if documents:
            for doc in documents:
                doc["_id"] = str(doc["_id"])
            return documents[0]  # ê°€ì¥ ìµœê·¼ í˜ì´ì§€ ë°˜í™˜
        return None

    except Exception as e:
        logger.error(f"âŒ ì‚¬ìš©ìë³„ ìµœê·¼ í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None


async def get_webhook_statistics(days: int = 7) -> Dict[str, Any]:
    """ì›¹í›… í˜¸ì¶œ í†µê³„ ì¡°íšŒ"""
    collection = get_meetup_collection("webhook_calls")

    # ì§€ì •ëœ ë‚ ì§œ ì´í›„ì˜ ë°ì´í„°ë§Œ ì¡°íšŒ
    since_date = datetime.now() - timedelta(days=days)

    pipeline = [
        {"$match": {"request_time": {"$gte": since_date}}},
        {
            "$group": {
                "_id": None,
                "total_calls": {"$sum": 1},
                "successful_calls": {"$sum": {"$cond": ["$success", 1, 0]}},
                "failed_calls": {"$sum": {"$cond": ["$success", 0, 1]}},
                "avg_processing_time": {"$avg": "$processing_time_ms"},
                "max_processing_time": {"$max": "$processing_time_ms"},
            }
        },
    ]

    results = await collection.aggregate(pipeline).to_list(1)

    if results:
        stats = results[0]
        stats["success_rate"] = (
            (stats["successful_calls"] / stats["total_calls"] * 100)
            if stats["total_calls"] > 0
            else 0
        )
        return stats
    else:
        return {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "success_rate": 0,
            "avg_processing_time": 0,
            "max_processing_time": 0,
        }


async def get_active_users(days: int = 30) -> List[Dict[str, Any]]:
    """í™œì„± ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ"""
    collection = get_meetup_collection("user_preferences")

    since_date = datetime.now() - timedelta(days=days)

    cursor = collection.find({"last_active": {"$gte": since_date}}).sort(
        "command_count", -1
    )

    documents = await cursor.to_list(length=100)
    for doc in documents:
        doc["_id"] = str(doc["_id"])

    return documents


# ===== í¸ì˜ í•¨ìˆ˜ë“¤ (Easy-to-use helper functions) =====


async def create_user_data_collection():
    """ì‚¬ìš©ì ë°ì´í„° ì»¬ë ‰ì…˜ ìƒì„± ì˜ˆì‹œ"""
    schema = {
        "bsonType": "object",
        "required": ["user_id", "username"],
        "properties": {
            "user_id": {"bsonType": "int", "description": "Discord user ID"},
            "username": {"bsonType": "string", "description": "Username"},
            "email": {"bsonType": "string", "description": "User email"},
            "preferences": {"bsonType": "object", "description": "User preferences"},
            "created_at": {"bsonType": "date", "description": "Creation timestamp"},
            "updated_at": {"bsonType": "date", "description": "Last update timestamp"},
        },
    }

    indexes = [
        {"keys": [("user_id", 1)], "options": {"unique": True}},
        {"keys": [("username", 1)], "options": {"unique": True}},
        {"keys": [("created_at", -1)], "options": {}},
    ]

    return await mongodb_connection.create_collection_with_schema(
        "user_data", schema, indexes
    )


async def create_meeting_logs_collection():
    """íšŒì˜ë¡ ë¡œê·¸ ì»¬ë ‰ì…˜ ìƒì„± ì˜ˆì‹œ"""
    schema = {
        "bsonType": "object",
        "required": ["meeting_id", "title", "date"],
        "properties": {
            "meeting_id": {"bsonType": "string", "description": "Meeting ID"},
            "title": {"bsonType": "string", "description": "Meeting title"},
            "date": {"bsonType": "date", "description": "Meeting date"},
            "attendees": {"bsonType": "array", "description": "List of attendees"},
            "content": {"bsonType": "string", "description": "Meeting content"},
            "action_items": {"bsonType": "array", "description": "Action items"},
            "created_at": {"bsonType": "date", "description": "Creation timestamp"},
            "updated_at": {"bsonType": "date", "description": "Last update timestamp"},
        },
    }

    indexes = [
        {"keys": [("meeting_id", 1)], "options": {"unique": True}},
        {"keys": [("date", -1)], "options": {}},
        {"keys": [("title", "text")], "options": {}},
    ]

    return await mongodb_connection.create_collection_with_schema(
        "meeting_logs", schema, indexes
    )


async def save_user_data(user_id: int, username: str, **additional_data):
    """ì‚¬ìš©ì ë°ì´í„° ìë™ ì €ì¥"""
    document = {"user_id": user_id, "username": username, **additional_data}
    return await mongodb_connection.auto_insert_document("user_data", document)


async def save_meeting_log(
    meeting_id: str, title: str, date: datetime, **additional_data
):
    """íšŒì˜ë¡ ìë™ ì €ì¥"""
    document = {
        "meeting_id": meeting_id,
        "title": title,
        "date": date,
        **additional_data,
    }
    return await mongodb_connection.auto_insert_document("meeting_logs", document)


async def save_custom_data(collection_name: str, data: Dict[str, Any]):
    """ì»¤ìŠ¤í…€ ë°ì´í„° ìë™ ì €ì¥ (ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìë™ ìƒì„±)"""
    return await mongodb_connection.auto_insert_document(collection_name, data)


async def bulk_save_data(collection_name: str, data_list: List[Dict[str, Any]]):
    """ëŒ€ëŸ‰ ë°ì´í„° ìë™ ì €ì¥"""
    return await mongodb_connection.auto_insert_many_documents(
        collection_name, data_list
    )


async def get_collection_data(
    collection_name: str, filter_query: Dict[str, Any] = None, limit: int = 100
):
    """ì»¬ë ‰ì…˜ì—ì„œ ë°ì´í„° ì¡°íšŒ"""
    try:
        collection = mongodb_connection.get_collection(collection_name)
        if filter_query is None:
            filter_query = {}

        cursor = collection.find(filter_query).limit(limit)
        documents = await cursor.to_list(length=limit)

        # ObjectIdë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        for doc in documents:
            if "_id" in doc:
                doc["_id"] = str(doc["_id"])

        return documents
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {collection_name}, ì˜¤ë¥˜: {e}")
        return []


# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜ë“¤
async def example_usage():
    """ì‚¬ìš©ë²• ì˜ˆì‹œ"""

    # 1. ìƒˆë¡œìš´ ì‚¬ìš©ì ë°ì´í„° ì €ì¥
    user_id = await save_user_data(
        user_id=123456789,
        username="john_doe",
        email="john@example.com",
        preferences={"theme": "dark", "notifications": True},
    )
    print(f"ì‚¬ìš©ì ë°ì´í„° ì €ì¥ë¨: {user_id}")

    # 2. íšŒì˜ë¡ ì €ì¥
    meeting_id = await save_meeting_log(
        meeting_id="meeting_2025_01_01",
        title="íŒ€ íšŒì˜",
        date=datetime.now(),
        attendees=["Alice", "Bob", "Charlie"],
        content="ì˜¤ëŠ˜ì˜ íšŒì˜ ë‚´ìš©...",
        action_items=["Task 1", "Task 2"],
    )
    print(f"íšŒì˜ë¡ ì €ì¥ë¨: {meeting_id}")

    # 3. ì»¤ìŠ¤í…€ ë°ì´í„° ì €ì¥
    custom_id = await save_custom_data(
        "my_custom_collection",
        {
            "type": "experiment",
            "name": "Test Data",
            "value": 42,
            "tags": ["test", "experiment", "data"],
        },
    )
    print(f"ì»¤ìŠ¤í…€ ë°ì´í„° ì €ì¥ë¨: {custom_id}")

    # 4. ëŒ€ëŸ‰ ë°ì´í„° ì €ì¥
    bulk_data = [{"name": f"Item {i}", "value": i * 10} for i in range(1, 101)]
    bulk_ids = await bulk_save_data("bulk_test_collection", bulk_data)
    print(f"ëŒ€ëŸ‰ ë°ì´í„° ì €ì¥ë¨: {len(bulk_ids)}ê°œ")

    # 5. ë°ì´í„° ì¡°íšŒ
    users = await get_collection_data("user_data", {"username": "john_doe"})
    print(f"ì¡°íšŒëœ ì‚¬ìš©ì: {users}")

    meetings = await get_collection_data("meeting_logs", limit=10)
    print(f"ìµœê·¼ íšŒì˜ë¡ {len(meetings)}ê°œ ì¡°íšŒë¨")
